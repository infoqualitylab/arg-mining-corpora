{
  "papers": [
    {
      "paper_id": "aeg_argumentative_essay_generation_via_a_dual_decoder_model_with_content_planning",
      "paper_title": "AEG: Argumentative Essay Generation via A Dual-Decoder Model with Content Planning",
      "authors": [
        "Jianzhu Bao",
        "Yasheng Wang",
        "Yitong Li",
        "Fei Mi",
        "Ruifeng X"
      ],
      "year": 2022,
      "paper_link": "https://aclanthology.org/2022.emnlp-main.343/",
      "doi": "https://doi.org/10.18653/v1/2022.emnlp-main.343",
      "open_alex_id": "https://openalex.org/W4385573259",
      "annotations": [
        {
          "corpus_id": "arg_essay",
          "annotation_task": [
            "None"
          ],
          "description": [
            "Unannotated"
          ],
          "annotator_type": "Unannotated",
          "agreement_type": "None",
          "agreement_score": "None",
          "accessibility": "Free",
          "corpus_link": "https://github.com/HITSZ-HLT/AEG",
          "corpus_name": "PARENT",
          "subset": 11282
        },
        {
          "corpus_id": "arg_essay",
          "annotation_task": [
            "Maximal Argument Quality Assessment"
          ],
          "description": [
            "Argument quality assessment:",
            "A dataset of prompt-essay pairs is used to evaluate the ability of several computational models in their performance of automatic essay generation. Part of the evaluation was performed human annotators, who gave scores from 1-5 (worst to best) for generated essays from 50 different prompts on quality dimensions of relevance, coherence, and content richness. No annotator agreement was reported, this human annotation is not publicly available, but the full ArgEssay dataset was made available."
          ],
          "annotator_type": "3 unspecified annotators",
          "agreement_type": "Not reported",
          "agreement_score": "Not reported",
          "accessibility": "Unavailable",
          "corpus_name": "PARENT",
          "subset": 50
        }
      ]
    },
    {
      "paper_id": "annotating_argument_components_and_relations_in_persuasive_essays",
      "paper_title": "Annotating Argument Components and Relations in Persuasive Essays",
      "authors": [
        "Christian Stab",
        "Iryna Gurevych"
      ],
      "year": 2014,
      "paper_link": "https://aclanthology.org/C14-1142/",
      "open_alex_id": "https://openalex.org/W2251931307",
      "annotations": [
        {
          "corpus_id": "aae_v1",
          "annotation_task": [
            "Argument Component Segmentation",
            "Argument Component Type Classification"
          ],
          "description": [
            "Argument component segmentation and classification:",
            "Essays are segmented into their argumentative components. Annotators first identified the major claim of the essay, and then the claims and premises in each paragraph. Because there are no predefined markables in the data, annotators had to determine the boundaries of argument components themselves, performing a segmentation of the argumentative text. A modification of Krippendorf’s α that allows for disagreement in span boundaries was used to capture annotation performance in both the segmentation of argumentative units and their classification."
          ],
          "annotator_type": "3 non-expert annotators",
          "agreement_type": "Krippendorff's αU",
          "agreement_score": {
            "Major Claim": 0.7726,
            "Claim": 0.6033,
            "Premise": 0.7594
          },
          "accessibility": "Free",
          "corpus_link": "https://tudatalib.ulb.tu-darmstadt.de/items/a52f7fef-4388-420c-91f0-ac03dac26bbb",
          "corpus_name": "PARENT",
          "subset": 90
        },
        {
          "corpus_id": "aae_v1",
          "annotation_task": [
            "Argument Relation Type Classification"
          ],
          "description": [
            "Argument component relation annotation:",
            "Classified argument components (premises, claims, and major claims) are linked by either support or attack relations. Annotators first linked claims and premises within each paragraph, before linking claims to major claims with either a support or attack relation."
          ],
          "annotator_type": "3 non-expert annotators",
          "agreement_type": "Krippendorff's α",
          "agreement_score": {
            "Support": 0.812,
            "Attack": 0.8066
          },
          "accessibility": "Free",
          "corpus_link": "https://tudatalib.ulb.tu-darmstadt.de/items/a52f7fef-4388-420c-91f0-ac03dac26bbb",
          "corpus_name": "PARENT",
          "subset": 90
        }
      ]
    },
    {
      "paper_id": "towards_feasible_guidelines_for_the_annotation_of_argument_schemes",
      "paper_title": "Towards Feasible Guidelines for the Annotation of Argument Schemes",
      "authors": [
        "Elena Musi",
        "Debanjan Ghosh",
        "Smaranda Muresan"
      ],
      "year": 2016,
      "paper_link": "https://aclanthology.org/W16-2810/",
      "doi": "https://doi.org/10.18653/v1/w16-2810",
      "open_alex_id": "https://openalex.org/W2508604498",
      "annotations": [
        {
          "corpus_id": "aae_v1",
          "annotation_task": [
            "Argument Type Identification"
          ],
          "description": [
            "Argumentum Model of Topics:",
            "A subset of 30 essays from AAE1 were annotated using an annotation scheme based on the Argumentum Model of Topics for a pilot study. Several student annotators were given a set of identification questions for a group of argument schemes from the Argumentum Model of Topics, and asked to identify which question most closely matches argumentative components linked by a support relation (annotators could select no argument)."
          ],
          "annotator_type": "9 graduate students with no specific background in linguistics or argumentation",
          "agreement_type": "Fleiss' κ",
          "agreement_score": 0.1,
          "accessibility": "Unavailable",
          "corpus_name": "AMT annotated essays",
          "subset": 30
        }
      ]
    },
    {
      "paper_id": "parsing_argumentation_structures_in_persuasive_essays",
      "paper_title": "Parsing Argumentation Structures in Persuasive Essays",
      "authors": [
        "Christian Stab",
        "Iryna Gurevych"
      ],
      "year": 2017,
      "paper_link": "https://aclanthology.org/J17-3005/",
      "doi": "https://doi.org/10.1162/coli_a_00295",
      "open_alex_id": "https://openalex.org/W2343649478",
      "annotations": [
        {
          "corpus_id": "aae_v2",
          "annotation_task": [
            "Argument Component Segmentation",
            "Argument Component Type Classification"
          ],
          "description": [
            "Coarse-grained argument component classification:",
            "402 persuasive essays were segmented into their argumentative components by annotators, which were then classified as either major claim, claim, or premise. Annotation was performed by three non-native speakers, one of which was an expert, and annotation was performed using the brat rapid annotation tool. Annotator agreement was reported for a random subset of 80 essays that all three annotators labelled, a modification of Krippendorf’s α that allows for disagreement in span boundaries was used to capture annotation performance in both the segmentation of argumentative units and their classification."
          ],
          "annotator_type": "3, 1 of which was an expert",
          "agreement_type": "Krippendorff's αU",
          "agreement_score": {
            "Major Claim": 0.81,
            "Claim": 0.584,
            "Premise": 0.824
          },
          "accessibility": "Free",
          "corpus_link": "https://tudatalib.ulb.tu-darmstadt.de/items/9177c48c-8bd5-4881-9cb4-0632b5941464",
          "corpus_name": "PARENT",
          "subset": 402
        },
        {
          "corpus_id": "aae_v2",
          "annotation_task": [
            "Argument Relation Identification",
            "Argument Relation Type Classification"
          ],
          "description": [
            "Argument component relation classification:",
            "A set of 4922 previously annotated argumentative units from 402 persuasive essays were annotated with their argumentative relations. Pairs were obtained by considering all pairs of argumentative components that occur within a paragraph as markable. All pairs were annotated by 3 annotators, who could either label relations as support or attack."
          ],
          "annotator_type": "3, 1 of which was an expert",
          "agreement_type": "Fleiss' κ",
          "agreement_score": {
            "Support": 0.708,
            "Attack": 0.737
          },
          "accessibility": "Free",
          "corpus_link": "https://tudatalib.ulb.tu-darmstadt.de/items/9177c48c-8bd5-4881-9cb4-0632b5941464",
          "corpus_name": "PARENT",
          "subset": 402
        }
      ]
    },
    {
      "paper_id": "recognizing_the_absence_of_opposing_arguments_in_persuasive_essays",
      "paper_title": "Recognizing the Absence of Opposing Arguments in Persuasive Essays",
      "authors": [
        "Christian Stab",
        "Iryna Gurevych"
      ],
      "year": 2016,
      "paper_link": "https://aclanthology.org/W16-2813/",
      "doi": "https://doi.org/10.18653/v1/w16-2813",
      "open_alex_id": "https://openalex.org/W2516916351",
      "annotations": [
        {
          "corpus_id": "aae_v2",
          "annotation_task": [
            "Other"
          ],
          "description": [
            "Myside bias:",
            "402 persuasive essays are annotated as either positive or negative, where positive indicates the say includes an opposing an argument, and negative if it only includes arguments supporting the author’s standpoint. The existing AAE2 corpus was used to derive initial document-level annotations, previously annotated argument structures include arguments annotated with support or attack relationships within essays and so could be re-used. To verify these automatically derived annotations, a subset of 80 essays were annotated by 3 independent annotators and agreement was reported which showed the derived annotations were reliable."
          ],
          "annotator_type": "3 unspecified annotators",
          "agreement_type": "Krippendorff's α",
          "agreement_score": 0.787,
          "accessibility": "Free",
          "corpus_link": "https://tudatalib.ulb.tu-darmstadt.de/items/ecb6f8cf-0bf3-4843-b299-8c1d9819cee9",
          "corpus_name": "Opposing Arguments in Persuasive Essays",
          "subset": 402
        }
      ]
    },
    {
      "paper_id": "recognizing_insufficiently_supported_arguments_in_argumentative_essays",
      "paper_title": "Recognizing Insufficiently Supported Arguments in Argumentative Essays",
      "authors": [
        "Christian Stab",
        "Iryna Gurevych"
      ],
      "year": 2017,
      "paper_link": "https://aclanthology.org/E17-1092/",
      "doi": "https://doi.org/10.18653/v1/e17-1092",
      "open_alex_id": "https://openalex.org/W2740760908",
      "annotations": [
        {
          "corpus_id": "aae_v2",
          "annotation_task": [
            "Minimal Argument Quality Assessment"
          ],
          "description": [
            "Sufficiency:",
            "Each paragraph from 402 persuasive essays was considered as an argument, resulting in 1029 arguments ready for annotation. Then, 3 non-native speakers with excellent English proficiency independently annotated all arguments as either sufficient (premises provide enough evidence to accept or reject the claim) or insufficient. An evaluation set of 433 arguments was annotated by all three annotators to compute agreement."
          ],
          "annotator_type": "3 non-native speakers with excellent English proficiency",
          "agreement_type": "Fleiss' κ",
          "agreement_score": 0.7672,
          "accessibility": "Free",
          "corpus_link": "https://tudatalib.ulb.tu-darmstadt.de/items/5d6a075e-697d-4f82-977f-b2cb16aa8542",
          "corpus_name": "Insufficiently Supported Arguments in Argumentative Essays",
          "subset": 402
        }
      ]
    },
    {
      "paper_id": "towards_fine_grained_argumentation_strategy_analysis_in_persuasive_essays",
      "paper_title": "Towards Fine-Grained Argumentation Strategy Analysis in Persuasive Essays",
      "authors": [
        "Robin Schaefer",
        "René Knaebel",
        "Manfred Stede"
      ],
      "year": 2023,
      "paper_link": "https://aclanthology.org/2023.argmining-1.8/",
      "doi": "https://doi.org/10.18653/v1/2023.argmining-1.8",
      "open_alex_id": "https://openalex.org/W4389518243",
      "annotations": [
        {
          "corpus_id": "aae_v2",
          "annotation_task": [
            "Argument Component Type Classification"
          ],
          "description": [
            "Fine-grained claim-type classification:",
            "402 essays were annotated with a fine-grained classification of their claims types. Claims in the essay had previously been extracted. Annotation was performed by 3 experts (including 1 author), who were able to label claims as one of 3 types: policy, value, or fact. Annotation proceeded in stages, with agreement computed at each stage to improve guidelines, final annotator agreement was computed over a set of 40 essays that all annotators labelled."
          ],
          "annotator_type": "3 experts",
          "agreement_type": "Krippendorff's α",
          "agreement_score": {
            "Policy": 0.78,
            "Value": 0.52,
            "Fact": 0.34,
            "Overall": 0.52
          },
          "accessibility": "Free",
          "corpus_link": "https://github.com/discourse-lab/arg-essays-semantic-types",
          "corpus_name": "Semantic Argument Types in Persuasive Essays",
          "subset": 402
        },
        {
          "corpus_id": "aae_v2",
          "annotation_task": [
            "Argument Component Type Classification"
          ],
          "description": [
            "Fine-grained premise-type classification:",
            "402 essays were annotated with a fine-grained classification of their premise types. Premises in the essay had previously been extracted in. Annotation was performed by 3 experts (including 1 author), who were able to label premises as one of 6 types: testimony, statistics, hypothetical-instance, real-example, common-ground, or other. Annotation proceeded in stages, with agreement computed at each stage to improve guidelines, final annotator agreement was computed over a set of 40 essays that all annotators labelled. The number of occurrences of the \"testimony\" and \"other\" class was too small to compute inter-annotator agreement."
          ],
          "annotator_type": "3 experts",
          "agreement_type": "Krippendorff's α",
          "agreement_score": {
            "Statistics": 0.16,
            "Hypothetical-Instance": 0.7,
            "Real-Example": 0.58,
            "Common-Ground": 0.42,
            "Overall": 0.53
          },
          "accessibility": "Free",
          "corpus_link": "https://github.com/discourse-lab/arg-essays-semantic-types",
          "corpus_name": "Semantic Argument Types in Persuasive Essayss",
          "subset": 402
        }
      ]
    },
    {
      "paper_id": "graph_embeddings_for_argumentation_quality_assessment",
      "paper_title": "Graph Embeddings for Argumentation Quality Assessment",
      "authors": [
        "Santiago Marro",
        "Elena Cabrio",
        "Serena Villata"
      ],
      "year": 2022,
      "paper_link": "https://aclanthology.org/2022.findings-emnlp.306/",
      "doi": "https://doi.org/10.18653/v1/2022.findings-emnlp.306",
      "open_alex_id": "https://openalex.org/W4315703085",
      "annotations": [
        {
          "corpus_id": "aae_v2",
          "annotation_task": [
            "Maximal Argument Quality Assessment"
          ],
          "description": [
            "Cogency and reasonableness:",
            "The arguments contained in 402 persuasive essays were annotated with scores for their cogency and reasonableness. An argument is cogent if it has individually acceptable premises that are relevant and sufficient to draw the arguments conclusions. An argument is reasonable if it contributes to the resolution of an issue in a way that is acceptable to the target audience. The previously annotated argument components and relations in the AEE2 corpus were used to aid in annotation. Cogency was scored on a 3-point scale (originally a 5-point scale which was reduced after experimentation) for the premises of a given argument. Reasonableness was computed on a 5-point scale for a whole argumentation graph, taking into consideration the reasonableness of counterarguments and rebuttals. Annotation was carried out by 3 English speakers who were experts in argument mining. Agreement was calculated over a subset of 33 essays."
          ],
          "annotator_type": "3 English speakers who were experts in argument mining",
          "agreement_type": "Fleiss' κ",
          "agreement_score": {
            "Cogency": 0.86,
            "Reasonableness Counterargument": 0.78,
            "Reasonableness Rebuttal": 0.84
          },
          "accessibility": "Free",
          "corpus_link": "https://aclanthology.org/2022.findings-emnlp.306/",
          "corpus_name": "Quality and strategy annotated essays",
          "subset": 402
        },
        {
          "corpus_id": "aae_v2",
          "annotation_task": [
            "Argument Type Identification"
          ],
          "description": [
            "Rhetorical strategy:",
            "402 essays were annotated at the argument level with their rhetorical strategy: either ethos, logos, or pathos. Annotation was carried out by 3 English speakers who were experts in argument mining. Agreement was calculated over a subset of 33 essays."
          ],
          "annotator_type": "3 English speakers who were experts in argument mining",
          "agreement_type": "Fleiss' κ",
          "agreement_score": 0.85,
          "accessibility": "Free",
          "corpus_link": "https://aclanthology.org/2022.findings-emnlp.306/",
          "corpus_name": "Quality and strategy annotated essays",
          "subset": 402
        }
      ]
    },
    {
      "paper_id": "zero_shot_stance_detection_a_dataset_and_model_using_generalized_topic_representations",
      "paper_title": "Zero-Shot Stance Detection: A Dataset and Model using Generalized Topic Representations",
      "authors": [
        "Emily Allaway",
        "Kathleen McKeown"
      ],
      "year": 2020,
      "paper_link": "https://aclanthology.org/2020.emnlp-main.717/",
      "doi": "https://doi.org/10.18653/v1/2020.emnlp-main.717",
      "open_alex_id": "https://openalex.org/W3091998006",
      "annotations": [
        {
          "corpus_id": "arc",
          "annotation_task": [
            "Claim Extraction with Stance Classification"
          ],
          "description": [
            "Stance classification:",
            "3365 comments on 304 topics are annotated with their stance, a subset of the ARC corpus. Annotators were first asked to list all topics related to the comment and then provided with an automatically generated topic for the comment. Topics were extracted heuristically partly based on the original stance-position annotations in the ARC corpus. If an annotator disagreed that a topic applied to a given comment they could correct it, otherwise each annotator proceeded to annotate the stance of the comment towards the topic. Stance was labelled on 5 point scale, which was then mapped to a 3-point pro/con/neutral label. Each topic-comment pair was annotated by 3 crowd workers (Mechanical Turk), and the gold label taken by majority vote. Poor quality annotations were filtered out manually and by using MACE. The resulting dataset contains three kinds of annotation: stance labels on extracted topics provided to annotators, labels on topics corrected by annotators, and lists of possible topics provided by the annotators for each comment."
          ],
          "annotator_type": "3 crowd workers per topic-comment pair",
          "agreement_type": "Krippendorff's α",
          "agreement_score": 0.427,
          "accessibility": "Free",
          "corpus_link": "https://github.com/emilyallaway/zero-shot-stance",
          "corpus_name": "VAST",
          "subset": 3365
        }
      ]
    },
    {
      "paper_id": "an_annotated_corpus_of_argumentative_microtexts",
      "paper_title": "An Annotated Corpus of Argumentative Microtexts",
      "authors": [
        "Andreas Peldszus",
        "Manfred Stede"
      ],
      "year": 2015,
      "paper_link": "https://peldszus.github.io/files/eca2015-preprint.pdf",
      "annotations": [
        {
          "corpus_id": "microtexts_p1",
          "annotation_task": [
            "Argument Component Segmentation",
            "Argument Component Type Classification",
            "Argument Relation Identification",
            "Argument Relation Type Classification"
          ],
          "description": [
            "Argumentation structures:",
            "112 short texts (written in German) were annotated by their argumentation structure based on Freeman’s theory of the macro-structure of argumentation. The annotation scheme argumentative roles, argumentative functions, and the identification of central claims, which together form a fully linked argumentative structure. Annotation was carried out on the original German version of the texts by 3 annotators, and agreement was computed over a subset of 8 texts."
          ],
          "annotator_type": "3 unspecified annotators",
          "agreement_type": "Fleiss' κ",
          "agreement_score": 0.83,
          "accessibility": "Free",
          "corpus_link": "https://angcl.ling.uni-potsdam.de/resources/argmicro.html",
          "corpus_name": "PARENT",
          "subset": 112
        }
      ]
    },
    {
      "paper_id": "a_multi_layer_annotated_corpus_of_argumentative_text_from_argument_schemes_to_discourse_relations",
      "paper_title": "A Multi-layer Annotated Corpus of Argumentative Text: From Argument Schemes to Discourse Relations",
      "authors": [
        "Elena Musi",
        "Tariq Alhindi",
        "Manfred Stede",
        "Leonard Kriese",
        "Smaranda Muresan",
        "Andrea Rocci"
      ],
      "year": 2018,
      "paper_link": "https://aclanthology.org/L18-1258/",
      "open_alex_id": "https://openalex.org/W2805780415",
      "annotations": [
        {
          "corpus_id": "microtexts_p1",
          "annotation_task": [
            "Argument Type Identification"
          ],
          "description": [
            "Argumentum Model of Topics:",
            "112 short texts were annotated with their argument schemes based on the Argumentum Model of Topics. Annotation involved two tasks, first annotators had to, given a support or rebut relation that already existed in the original Microtexts corpus, identify the argument scheme among 8 possible schemes (or none if no reasoning was present). Second, annotators had to identify the associated inference rule of any identified argument scheme, these inference rules have forms such as \"if the effects the case, the cause is probably the case\" or \"if the cause is the case, the effect is the case\". Annotation was performed by 6 annotators (all students with relevant backgrounds), and agreement was computed over two sets of 20 microtexts that had each been annotated by 3 annotators."
          ],
          "annotator_type": "6 annotators, including 4 students with a background in linguistics and argumentation, and 2 PhD students with a background in natural language processing",
          "agreement_type": "Fleiss' κ",
          "agreement_score": 0.296,
          "accessibility": "Free",
          "corpus_link": "https://angcl.ling.uni-potsdam.de/resources/argmicro.html",
          "corpus_name": "AMT annotated microtexts",
          "subset": 112
        }
      ]
    },
    {
      "paper_id": "more_or_less_controlled_elicitation_of_argumentative_text_Enlarging_a_microtext_corpus_via-crowdsourcing",
      "paper_title": "More or less controlled elicitation of argumentative text: Enlarging a microtext corpus via crowdsourcing",
      "authors": [
        "Maria Skeppstedt",
        "Andreas Peldszus",
        "Manfred Stede."
      ],
      "year": 2018,
      "paper_link": "https://aclanthology.org/W18-5218/",
      "doi": "https://doi.org/10.18653/v1/w18-5218",
      "open_alex_id": "https://openalex.org/W2898653207",
      "annotations": [
        {
          "corpus_id": "microtexts_p2",
          "annotation_task": [
            "Argument Component Segmentation",
            "Argument Component Type Classification",
            "Argument Relation Identification",
            "Argument Relation Type Classification"
          ],
          "description": [
            "Argumentation structures:",
            "171 short texts were annotated with full argumentative structure by 2 annotators. Annotator agreement was not computed, as annotators followed a similar annotation scheme to the first version of the argumentative microtexts corpus. Their annotation resulted in the identification of 932 argumentative units, connected by 467 convergent support relations, 23 example support relations, 137 rebutting attack relations, 77 undercutting attack relations, 57 linked support or attack relations, and 29 restatement relations. Notably, restatement relations did not occur in the first version of the microtexts corpus."
          ],
          "annotator_type": "2 annotators, one who was a co-author of the work",
          "agreement_type": "Not reported",
          "agreement_score": "Not reported",
          "accessibility": "Free",
          "corpus_link": "https://angcl.ling.uni-potsdam.de/resources/argmicro.html",
          "corpus_name": "PARENT",
          "subset": 171
        }
      ]
    },
    {
      "paper_id": "learning_from_revisions_quality_assessment_of_claims_in_argumentation_at_scale",
      "paper_title": "Learning From Revisions: Quality Assessment of Claims in Argumentation at Scale",
      "authors": [
        "Gabriella Skitalinskaya",
        "Jonas Klaff",
        "Henning Wachsmuth"
      ],
      "year": 2021,
      "paper_link": "https://aclanthology.org/2021.eacl-main.147/",
      "doi": "https://doi.org/10.18653/v1/2021.eacl-main.147",
      "open_alex_id": "https://openalex.org/W3122759107",
      "annotations": [
        {
          "corpus_id": "claimrev_v1",
          "annotation_task": [
            "None"
          ],
          "description": [
            "Unannotated"
          ],
          "annotator_type": "Unannotated",
          "agreement_type": "None",
          "agreement_score": "None",
          "accessibility": "Upon Request",
          "corpus_link": "https://github.com/GabriellaSky/claimrev",
          "corpus_name": "PARENT",
          "subset": 124312
        },
        {
          "corpus_id": "claimrev_v1",
          "annotation_task": [
            "Maximal Argument Quality Assessment"
          ],
          "description": [
            "Relative quality assessment:",
            "A randomly sampled set of 315 claim revision pairs was annotated by the authors to test the hypothesis that later versions have an increase in quality to previous versions. Their hypothesis was supported with the results that 93% of later versions had an improved overall argument quality in comparison to previous versions."
          ],
          "annotator_type": "2 authors of the paper (experts)",
          "agreement_type": "Cohen's κ",
          "agreement_score": 0.75,
          "accessibility": "Upon Request",
          "corpus_link": "https://github.com/GabriellaSky/claimrev",
          "corpus_name": "PARENT",
          "subset": 315
        },
        {
          "corpus_id": "claimrev_v1",
          "annotation_task": [
            "Other"
          ],
          "description": [
            "Revision label agreement:",
            "A random sampled set of 440 claim revision pairs were annotated by authors of the paper by their type (such as \"Claim Clarification\") to compare with original labels provided by kialo. Agreement was reported between the annotators, and between the annotators and the original labels."
          ],
          "annotator_type": "2 authors of the paper (experts)",
          "agreement_type": "Cohen's κ",
          "agreement_score": {
            "Annotator agreement": 0.84,
            "Agreement with original labels": 0.82
          },
          "accessibility": "Upon Request",
          "corpus_link": "https://github.com/GabriellaSky/claimrev",
          "corpus_name": "PARENT",
          "subset": 440
        },
        {
          "corpus_id": "claimrev_v1",
          "annotation_task": [
            "Maximal Argument Quality Assessment"
          ],
          "description": [
            "Taxonomical argument quality assessment:",
            "A randomly sampled set of 315 claim revision pairs was annotated by the authors to explore whether the type of revision had an effect on one of 15 argument quality dimensions. The authors observed strong correlations between some revision types and quality dimensions (such as \"Corrected/Added Links\" and the logical dimensions of \"Cogency\" and \"Local Sufficiency\")."
          ],
          "annotator_type": "1 author of the paper (expert)",
          "agreement_type": "None",
          "agreement_score": "None",
          "accessibility": "Upon Request",
          "corpus_link": "https://github.com/GabriellaSky/claimrev",
          "corpus_name": "PARENT",
          "subset": 315
        }
      ]
    },
    {
      "paper_id": "claim_optimization_in_computational_argumentation",
      "paper_title": "Claim Optimization in Computational Argumentation",
      "authors": [
        "Gabriella Skitalinskaya",
        "Maximilian Spliethöver",
        "Henning Wachsmuth"
      ],
      "year": 2023,
      "paper_link": "https://aclanthology.org/2023.inlg-main.10/",
      "doi": "https://doi.org/10.18653/v1/2023.inlg-main.10",
      "open_alex_id": "https://openalex.org/W4389009581",
      "annotations": [
        {
          "corpus_id": "claimrev_v2",
          "annotation_task": [
            "None"
          ],
          "description": [
            "Unannotated"
          ],
          "annotator_type": "Unannotated",
          "agreement_type": "None",
          "agreement_score": "None",
          "accessibility": "Free",
          "corpus_link": "https://github.com/GabriellaSky/claim_optimization",
          "corpus_name": "PARENT",
          "subset": 198089
        }
      ]
    },
    {
      "paper_id": "a_corpus_of_erulemaking_user_comments_for_measuring_evaluability_of_arguments",
      "paper_title": "A Corpus of eRulemaking User Comments for Measuring Evaluability of Arguments",
      "authors": [
        "Joonsuk Park",
        "Claire Cardie"
      ],
      "year": 2018,
      "paper_link": "https://aclanthology.org/L18-1257/",
      "open_alex_id": "https://openalex.org/W2807191971",
      "annotations": [
        {
          "corpus_id": "cornell_erulemaking",
          "annotation_task": [
            "Argument Component Type Classification"
          ],
          "description": [
            "Elementary unit classification:",
            "A dataset of 731 comments were annotated at the sentence/clause level with their argument components, resulting in 4931 argument components in the final labelling. Argument components were one of 4 \"Elementary Units\": fact, testimony, value, policy, or reference. Annotation was performed by 2 annotators, while a 3rd annotator was used to resolve disagreements and produce the final gold label. Annotator agreement was reported between the 2 primary annotators."
          ],
          "annotator_type": "2 unspecified annotators",
          "agreement_type": "Krippendorff's α",
          "agreement_score": 0.648,
          "accessibility": "Free",
          "corpus_link": "https://huggingface.co/datasets/DFKI-SLT/cdcp",
          "corpus_name": "PARENT",
          "subset": 731
        },
        {
          "corpus_id": "cornell_erulemaking",
          "annotation_task": [
            "Argument Relation Identification",
            "Argument Relation Type Classification"
          ],
          "description": [
            "Support relation classification:",
            "A dataset of 731 comments that were previously annotated with their argument components, were annotated again with the relation between their argument components, resulting in 1221 relations in the final labelling. Argument relations were one of 2 support relations: reason or evidence. Annotation was performed by 2 annotators, while a 3rd annotator was used to resolve disagreements and produce the final gold label. Annotator agreement was reported between the 2 primary annotators."
          ],
          "annotator_type": "2 unspecified annotators",
          "agreement_type": "Krippendorff's α",
          "agreement_score": 0.441,
          "accessibility": "Free",
          "corpus_link": "https://huggingface.co/datasets/DFKI-SLT/cdcp",
          "corpus_name": "PARENT",
          "subset": 731
        }
      ]
    },
    {
      "paper_id": "an_argument_annotated_corpus_of_scientific_publications",
      "paper_title": "An Argument-Annotated Corpus of Scientific Publications",
      "authors": [
        "Anne Lauscher",
        "Goran Glavaš",
        "Simone Paolo Ponzetto"
      ],
      "year": 2018,
      "paper_link": "https://aclanthology.org/W18-5206/",
      "doi": "https://doi.org/10.18653/v1/w18-5206",
      "open_alex_id": "https://openalex.org/W2898734253",
      "annotations": [
        {
          "corpus_id": "dr_inventor",
          "annotation_task": [
            "Argument Component Type Classification"
          ],
          "description": [
            "Toulmin-style component classification:",
            "Scientific publications were annotated with their argumentative components by 4 annotators (1 expert and 3 non-expert). Component types were based on the Toulmin model and a preliminary study, resulting in three argumentative components: own claim, background claim, and data. Annotator agreement was computed using F1, and was reported in a bar chart that gives approximate values. Annotator agreement was computed for 5 iterations of the task, and for both a weak and strict interpretation of the spans of the argument components."
          ],
          "annotator_type": "1 expert and 3 non-expert annotators",
          "agreement_type": "F1",
          "agreement_score": {
            "Approximate 5th iteration weak": 0.75,
            "Approximate 5th iteration strict": 0.6
          },
          "accessibility": "Free",
          "corpus_link": "http://data.dws.informatik.uni-mannheim.de/sci-arg/compiled_corpus.zip",
          "corpus_name": "SciArg",
          "subset": 40
        },
        {
          "corpus_id": "dr_inventor",
          "annotation_task": [
            "Argument Relation Identification",
            "Argument Relation Type Classification"
          ],
          "description": [
            "Dung-style relation classification:",
            "previously annotated argument components from 40 scientific publications were annotated with their relation to each other by 4 annotators (1 expert and 3 non-expert). Relations were based on those described in Dung (1995) and included the three relations of supports, contradicts, and semantically the same (argument coreference). Annotator agreement was computed using F1, and was reported in a bar chart that gives approximate values. Annotator agreement was computed for 5 iterations of the task, and for both a weak and strict interpretation of the spans of the argument components."
          ],
          "annotator_type": "1 expert and 3 non-expert annotators",
          "agreement_type": "F1",
          "agreement_score": {
            "Approximate 5th iteration weak": 0.45,
            "Approximate 5th iteration strict": 0.35
          },
          "accessibility": "Free",
          "corpus_link": "http://data.dws.informatik.uni-mannheim.de/sci-arg/compiled_corpus.zip",
          "corpus_name": "SciArg",
          "subset": 40
        }
      ]
    },
    {
      "paper_id": "ethix_a_dataset_for_argument_scheme_classification_in_ethical_debates",
      "paper_title": "EthiX: A Dataset for Argument Scheme Classification in Ethical Debates",
      "authors": [
        "Elfia Bezou-Vrakatseli",
        "Oana Cocarascu",
        "Sanjay Modgil"
      ],
      "year": 2024,
      "paper_link": "https://ebooks.iospress.nl/DOI/10.3233/FAIA240919",
      "doi": "https://doi.org/10.3233/faia240919",
      "open_alex_id": "https://openalex.org/W4403487202",
      "annotations": [
        {
          "corpus_id": "ethix",
          "annotation_task": [
            "Argument Type Identification"
          ],
          "description": [
            "Argument scheme identification:",
            "Arguments are annotated arguments with their argument scheme according to Walton's taxonomy (e.g., argument from example, values, positive/negative consequences, cause-to-effect, expert opinion, alternatives, analogy), with Argument Scheme Key (ASK) guidelines to aid consistent labeling. Annotation was performed by the authors, and agreement measured with another PhD student who was not an expert in argumentation."
          ],
          "annotator_type": "Authors and a PhD student in explainable AI, trained with workshops and examples",
          "agreement_type": "Cohen's κ",
          "agreement_score": 0.523,
          "accessibility": "Free",
          "corpus_link": "https://github.com/ElfiaBv/EthiX",
          "corpus_name": "PARENT",
          "subset": 686
        }
      ]
    },
    {
      "paper_id": "creating_a_domain_diverse_corpus_for_theory_based_argument_quality_assessment",
      "paper_title": "Creating a Domain-diverse Corpus for Theory-based Argument Quality Assessment",
      "authors": [
        "Lily Ng",
        "Anne Lauscher",
        "Joel Tetreault",
        "Courtney Napoles"
      ],
      "year": 2020,
      "paper_link": "https://aclanthology.org/2020.argmining-1.13/",
      "doi": "https://doi.org/10.48550/arxiv.2011.01589",
      "open_alex_id": "https://openalex.org/W4287608578",
      "annotations": [
        {
          "corpus_id": "gaq",
          "annotation_task": [
            "Maximal Argument Quality Assessment"
          ],
          "description": [
            "Theory-driven argument quality assessment: ",
            "Arguments are annotated along 3 argument quality dimensions (cogency, reasonableness, and effectiveness) as well as for overall quality along a 5 point scale (very low/low/medium/high/very high quality or cannot judge). Subdimensions of the 3 argument quality dimensions are not scored, but are used to guide annotation. Annotation was performed by two groups, a set of four expert annotators with a deep understanding of argumentation theory, and a set of crowd annotators who were fluent in English. Agreement was measured along each quality dimension by comparing annotations between the set of experts and set of crowd annotators in arguments that were annotated by both groups within each category of text (CQA or debate or review)."
          ],
          "annotator_type": "4 expert annotators, and an unspecified number of crowd annotators",
          "agreement_type": "Krippendorff's α",
          "agreement_score": {
            "CQA Cogency": 0.42,
            "CQA Effectiveness": 0.52,
            "CQA Reasonableness": 0.52,
            "CQA Overall": 0.53,
            "Debates Cogency": 0.14,
            "Debates Effectiveness": 0.11,
            "Debates Reasonableness": 0.21,
            "Debates Overall": 0.19,
            "Reviews Cogency": 0.32,
            "Reviews Effectiveness": 0.32,
            "Reviews Reasonableness": 0.31,
            "Reviews Overall": 0.33
          },
          "accessibility": "Free",
          "corpus_link": "https://github.com/grammarly/gaqcorpus",
          "corpus_name": "PARENT",
          "subset": 5285
        }
      ]
    },
    {
      "paper_id": "a_large_scale_dataset_for_argument_quality_ranking_construction_and_analysis",
      "paper_title": "A Large-scale Dataset for Argument Quality Ranking: Construction and Analysis",
      "authors": [
        "Shai Gretz",
        "Roni Friedman",
        "Edo Cohen-Karlik",
        "Assaf Toledo",
        "Dan Lahav",
        "Ranit Aharonov",
        "Noam Slonim"
      ],
      "year": 2020,
      "paper_link": "https://ojs.aaai.org/index.php/AAAI/article/view/6285",
      "doi": "https://doi.org/10.1609/aaai.v34i05.6285",
      "open_alex_id": "https://openalex.org/W2991389267",
      "annotations": [
        {
          "corpus_id": "ibm_rank_30k",
          "annotation_task": [
            "Maximal Argument Quality Assessment"
          ],
          "description": [
            "Individual Binary Argument Quality:",
            "Arguments from are annotated with a binary quality score (1 or 0) by crowd annotators. 10 annotators annotated each argument, with an answer of yes/no (1 or 0) if they would recommend the argument be used as is by a person in a hypothetical speech on the topic. Poorly performing annotators, and those who failed test questions, were filtered out. To obtain a continuous quality score, scores were aggregated using two methods, MACE-P and WA (details in paper). Annotator agreement by crowd annotators is not reported in the paper, however the continuous scoring functions (MACE-P and WA) were assessed using several methods that compare predicted scores to a gold-standard pairwise annotation."
          ],
          "annotator_type": "10 crowd annotators per argument",
          "agreement_type": "Not reported",
          "agreement_score": "Not reported",
          "accessibility": "Free",
          "corpus_link": "https://huggingface.co/datasets/ibm-research/argument_quality_ranking_30k",
          "corpus_name": "PARENT",
          "subset": 30497
        }
      ]
    },
    {
      "paper_id": "a_benchmark_dataset_for_automatic_detection_of_claims_and-evidence_in_the_context_of_controversial_topics",
      "paper_title": "A Benchmark Dataset for Automatic Detection of Claims and Evidence in the Context of Controversial Topics",
      "authors": [
        "Ehud Aharoni",
        "Anatoly Polnarov",
        "Tamar Lavee",
        "Daniel Hershcovich",
        "Ran Levy",
        "Ruty Rinott",
        "Dan Gutfreund",
        "Noam Slonim"
      ],
      "year": 2014,
      "paper_link": "https://aclanthology.org/W14-2109/",
      "doi": "https://doi.org/10.3115/v1/w14-2109",
      "open_alex_id": "https://openalex.org/W2251448848",
      "annotations": [
        {
          "corpus_id": "ibm_ce_v1",
          "annotation_task": [
            "Argument Component Segmentation",
            "Argument Component Type Classification",
            "Argument Summarization"
          ],
          "description": [
            "Claim and evidence identification:",
            "586 Wikipedia articles were annotated to derive a set of 2683 argument elements made up of context-dependent claims and context-dependent evidence. The annotation process was meticulous, and involved 20 well-trained annotators who followed several steps. First, 5 annotators in a search team were given a topic and independently searched for related articles. Second, 5 annotators independently detected candidate context-dependent claims from the articles. Third, in a \"Claim confirmation\" stage, 5 annotators examined these candidates and rejected/accepted them (agreement was reported for this stage). In a fourth stage, 5 annotators detected candidate context-dependent evidence related to accepted context-dependent claims. In a fifth stage, \"Evidence confirmation\" was performed in the same manner as \"Claim confirmation\", with 5 annotators accepting/rejecting candidate context-dependent evidence."
          ],
          "annotator_type": "20 well-trained annotators",
          "agreement_type": "Cohen's κ",
          "agreement_score": {
            "Claim confirmation": 0.39,
            "Evidence confirmation": 0.4
          },
          "accessibility": "Free",
          "corpus_link": "https://research.ibm.com/haifa/dept/vst/debating_data.shtml",
          "corpus_name": "PARENT",
          "subset": 586
        }
      ]
    },
    {
      "paper_id": "show_me_your_evidence_an_automatic_method_for_context_dependent_evidence_detection",
      "paper_title": "Show Me Your Evidence - an Automatic Method for Context Dependent Evidence Detection",
      "authors": [
        "Ruty Rinott",
        "Lena Dankin",
        "Carlos Alzate Perez",
        "Mitesh M. Khapra",
        "Ehud Aharoni",
        "Noam Slonim"
      ],
      "year": 2015,
      "paper_link": "https://aclanthology.org/D15-1050/",
      "doi": "https://doi.org/10.18653/v1/d15-1050",
      "open_alex_id": "https://openalex.org/W2250762536",
      "annotations": [
        {
          "corpus_id": "ibm_ce_v2",
          "annotation_task": [
            "Argument Component Segmentation",
            "Argument Component Type Classification",
            "Argument Summarization"
          ],
          "description": [
            "Evidence identification:",
            "547 Wikipedia articles were annotated to derive a set of context-dependent evidence. In the first stage, 5 annotators read the article and marked all context-dependent evidence candidates, in a second stage, 5 annotators confirmed or rejected each candidate and performed a fine-grained classification of the type of evidence (study, expert or anecdotal). Context-dependent evidence candidates and their type were determined by majority vote."
          ],
          "annotator_type": "15 annotators involved across all stages",
          "agreement_type": "Not reported",
          "agreement_score": "Not reported",
          "accessibility": "Free",
          "corpus_link": "https://research.ibm.com/haifa/dept/vst/debating_data.shtml",
          "corpus_name": "PARENT",
          "subset": 547
        }
      ]
    },
    {
      "paper_id": "stance_classification_of_context_dependent_claims",
      "paper_title": "Stance Classification of Context-Dependent Claims",
      "authors": [
        "Roy Bar-Haim",
        "Indrajit Bhattacharya",
        "Francesco Dinuzzo",
        "Amrita Saha",
        "Noam Slonim"
      ],
      "year": 2017,
      "paper_link": "https://aclanthology.org/E17-1024/",
      "doi": "https://doi.org/10.18653/v1/e17-1024",
      "open_alex_id": "https://openalex.org/W2739836857",
      "annotations": [
        {
          "corpus_id": "ibm_cs",
          "annotation_task": [
            "Argument Component Segmentation",
            "Claim Extraction with Stance Classification"
          ],
          "description": [
            "Claim stance classification:",
            "First each of the 55 topics in the dataset were annotated by one of the authors for its target and sentiment. Then for the main annotation task, each claim was independently labelled with stance by 5 annotators who were provided with the target of the topic. Annotators first identified the target (positive/negative assertion) and sentiment of the claim, before determining if the claim target was the same or contrasted with the topic target. To obtain a final gold label, overlapping claim targets were clustered together, if no cluster contained the majority of the annotations then the claim was labelled as incompatible, if a majority was found then the minority annotations were discarded and the majority labels selected. No annotator agreement was reported, but the majority luster was found for 98.5% of annotated claims, and for 92.5% of the claims most of the annotators agreed on the exact boundaries of the target."
          ],
          "annotator_type": "At least 5 annotators labelled each claim",
          "agreement_type": "Not reported",
          "agreement_score": "Not reported",
          "accessibility": "Free",
          "corpus_link": "https://research.ibm.com/haifa/dept/vst/debating_data.shtml",
          "corpus_name": "PARENT",
          "subset": 2394
        }
      ]
    },
    {
      "paper_id": "iam_a_comprehensive_and_large_scale_dataset_for_integrated_argument_mining_tasks",
      "paper_title": "IAM: A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks",
      "authors": [
        "Liying Cheng",
        "Lidong Bing",
        "Ruidan He",
        "Qian Yu",
        "Yan Zhang",
        "Luo Si"
      ],
      "year": 2022,
      "paper_link": "https://aclanthology.org/2022.acl-long.162/",
      "doi": "https://doi.org/10.18653/v1/2022.acl-long.162",
      "open_alex_id": "https://openalex.org/W4221151164",
      "annotations": [
        {
          "corpus_id": "iam",
          "annotation_task": [
            "Argument Component Segmentation",
            "Argument Component Type Classification",
            "Claim Extraction with Stance Classification",
            "Argument Relation Identification"
          ],
          "description": [
            "Claim and evidence identification:",
            "69,666 sentences from 1010 Wikipedia articles were annotated with a scheme to support a variety of argument mining tasks. The annotation process was two-staged: first, annotators identified claims and their stances (+1 for supporting, -1 for contesting the topic), and second, annotators linked claims to evidence through claim-evidence pairs (CEPs). Evidence was searched within 10 to 15 sentences preceding and after the claim."
          ],
          "annotator_type": "Each sentence was labelled by 2 paid professional data annotators. A third professional annotator was also used in a confirmation phase to resolve disagreement.",
          "agreement_type": "Cohen's κ",
          "agreement_score": 0.44,
          "accessibility": "Free",
          "corpus_link": "https://github.com/LiyingCheng95/IAM",
          "corpus_name": "PARENT",
          "subset": 1010
        }
      ]
    },
    {
      "paper_id": "from_arguments_to_key_points_towards_automatic_argument_summarization",
      "paper_title": "From Arguments to Key Points: Towards Automatic Argument Summarization",
      "authors": [
        "Roy Bar-Haim",
        "Lilach Eden",
        "Roni Friedman",
        "Yoav Kantor",
        "Dan Lahav",
        "Noam Slonim"
      ],
      "year": 2020,
      "paper_link": "https://aclanthology.org/2020.acl-main.371/",
      "doi": "https://doi.org/10.18653/v1/2020.acl-main.371",
      "open_alex_id": "https://openalex.org/W3021632220",
      "annotations": [
        {
          "corpus_id": "ibm_rank_30k",
          "annotation_task": [
            "Argument Summarization"
          ],
          "description": [
            "Key point assignment:",
            "A set of 7000 arguments was mapped to a set of 378 key points (curated by an expert on the 28 topics the arguments addressed). Each argument was labelled by 8 crowd annotators from Figure Eight, who were asked to check all relevant key points for the argument (or none). Various quality control measures were employed (such as test questions, and relying on annotators used in previous work). Inter-annotator agreement for the crowd annotators was reported, along with the agreement of 100 of the final key point labelings with an expert."
          ],
          "annotator_type": "8 crowd annotators per argument, 1 expert for final evaluation",
          "agreement_type": "Cohen's κ",
          "agreement_score": {
            "Crowd": 0.5,
            "Expert": 0.82
          },
          "accessibility": "Free",
          "corpus_link": "https://research.ibm.com/haifa/dept/vst/debating_data.shtml",
          "corpus_name": "KPA shared task dataset",
          "subset": 7000
        }
      ]
    },
    {
      "paper_id": "analyzing_argumentative_discourse_units_in_online_interactions",
      "paper_title": "Analyzing Argumentative Discourse Units in Online Interactions",
      "authors": [
        "Debanjan Ghosh",
        "Smaranda Muresan",
        "Nina Wacholder",
        "Mark Aakhus",
        "Matthew Mitsui"
      ],
      "year": 2014,
      "paper_link": "https://aclanthology.org/W14-2106/",
      "doi": "https://doi.org/10.3115/v1/w14-2106",
      "open_alex_id": "https://openalex.org/W2251521080",
      "annotations": [
        {
          "corpus_id": "language_of_opposition",
          "annotation_task": [
            "Argument Component Segmentation",
            "Argument Component Type Classification",
            "Argument Relation Identification"
          ],
          "description": [
            "Callouts and Targets:",
            "404 posts/comments are annotated according to a scheme based on Pragmatic Argumentation Theory (PAT), which frames arguments as interactions between Callouts and Targets. A Callout is a subsequent action that expresses an explicit stance or rationale toward a Target, defined as a prior action in the discussion. Five graduate students with strong humanities backgrounds were trained as annotators to perform an annotation study that followed a three-step process: (1) segmenting the text into Argumentative Discourse Units (ADUs), (2) classifying each segment as a Callout or Target, and (3) linking each Callout to its most recent Target. Agreement is reported separately for each topic in the corpus. The agreement reported below is agreement across the whole annotation task (segmentation and type classification)."
          ],
          "annotator_type": "5 graduate students with strong humanities backgrounds",
          "agreement_type": "Krippendorff's α",
          "agreement_score": {
            "Android": 0.64,
            "iPad": 0.73,
            "Layoffs": 0.87,
            "Twitter": 0.82
          },
          "accessibility": "Free",
          "corpus_link": "https://salts.rutgers.edu/identifying-the-language-of-opposition-in-online-interactions/",
          "corpus_name": "PARENT",
          "subset": 404
        },
        {
          "corpus_id": "language_of_opposition",
          "annotation_task": [
            "Argument Relation Type Classification",
            "Claim Extraction with Stance Classification"
          ],
          "description": [
            "Stance/Relation Classification:",
            "404 posts/comments are annotated according to a scheme based on Pragmatic Argumentation Theory (PAT), which frames arguments as interactions between Callouts and Targets. A Callout is a subsequent action that expresses an explicit stance or rationale toward a Target, defined as a prior action in the discussion. Five graduate students with strong humanities backgrounds were trained as annotators to perform the annotation. In this task, annotators had to perform a fine-grained classification of relations between callouts and targets, as either agree, disagree, or other. A range of agreement values across threads was reported."
          ],
          "annotator_type": "5 graduate students with strong humanities backgrounds",
          "agreement_type": "Fleiss' κ",
          "agreement_score": [
            0.45,
            0.55
          ],
          "accessibility": "Free",
          "corpus_link": "https://salts.rutgers.edu/identifying-the-language-of-opposition-in-online-interactions/",
          "corpus_name": "PARENT",
          "subset": 404
        },
        {
          "corpus_id": "language_of_opposition",
          "annotation_task": [
            "Claim Extraction with Stance Classification"
          ],
          "description": [
            "Rationale Identification:",
            "404 posts/comments are annotated according to a scheme based on Pragmatic Argumentation Theory (PAT), which frames arguments as interactions between Callouts and Targets. A Callout is a subsequent action that expresses an explicit stance or rationale toward a Target, defined as a prior action in the discussion. Five graduate students with strong humanities backgrounds were trained as annotators to perform the annotation.  In this task, annotators had to identify the stance and rationale of callouts towards their targets, including the exact boundaries of the text segments. No agreement reported, although for 50% of callouts the stance and rationale were easily identified by the annotators."
          ],
          "annotator_type": "5 graduate students with strong humanities background",
          "agreement_type": "Not reported",
          "agreement_score": "Not reported",
          "accessibility": "Free",
          "corpus_link": "https://salts.rutgers.edu/identifying-the-language-of-opposition-in-online-interactions/",
          "corpus_name": "PARENT",
          "subset": 404
        }
      ]
    },
    {
      "paper_id": "logical_fallacy_detection",
      "paper_title": "Logical Fallacy Detection",
      "authors": [
        "Zhijing Jin",
        "Abhinav Lalwani",
        "Tejas Vaidhya",
        "Xiaoyu Shen",
        "Yiwen Ding",
        "Zhiheng Lyu",
        "Mrinmaya Sachan",
        "Rada Mihalcea",
        "Bernhard Schölkopf"
      ],
      "year": 2022,
      "paper_link": "https://aclanthology.org/2022.findings-emnlp.532/",
      "doi": "https://doi.org/10.18653/v1/2022.findings-emnlp.532",
      "open_alex_id": "https://openalex.org/W4385567117",
      "annotations": [
        {
          "corpus_id": "logic",
          "annotation_task": [
            "None"
          ],
          "description": [
            "Unannotated"
          ],
          "annotator_type": "Unannotated",
          "agreement_type": "None",
          "agreement_score": "None",
          "accessibility": "Free",
          "corpus_link": "https://github.com/causalNLP/logical-fallacy",
          "corpus_name": "PARENT",
          "subset": 2449
        },
        {
          "corpus_id": "logic_climate",
          "annotation_task": [
            "Argument Component Segmentation",
            "Minimal Argument Quality Assessment"
          ],
          "description": [
            "Fallacy detection:",
            "Climate change news articles are annotated with text spans that contain logical fallacies. Two annotators who were native English speakers, worked through every sentence in each article, and label all logical fallacies where applicable. When labelling a text span as a logical fallacy, annotators had to pick between 1 of 13 possible logical fallacies. For the final ground-truth label, annotations were merged (text-spans), and where there was a divergent opinion, annotators consulted expert reviews on the site. If disagreement continued annotators had a discussion to resolve the divergent label. Annotator agreement was not reported, but prior to the annotation task the annotators achieved an accuracy of 85% on a test set."
          ],
          "annotator_type": "2 expert trained annotators who are native English speakers",
          "agreement_type": "Not reported",
          "agreement_score": "Not reported",
          "accessibility": "Free",
          "corpus_link": "https://github.com/causalNLP/logical-fallacy",
          "corpus_name": "PARENT",
          "subset": 1079
        }
      ]
    },
    {
      "paper_id": "nlas_multi_a_multilingual_corpus_of_automatically_generated_natural_language_argumentation_schemes",
      "paper_title": "NLAS-multi: A multilingual corpus of automatically generated Natural Language Argumentation Schemes",
      "authors": [
        "Ramon Ruiz-Dolz",
        "Joaquin Taverner",
        "John Lawrence",
        "Chris Reed"
      ],
      "year": 2024,
      "paper_link": "https://www.sciencedirect.com/science/article/pii/S2352340924010497",
      "doi": "https://doi.org/10.1016/j.dib.2024.111087",
      "open_alex_id": "https://openalex.org/W4403913558",
      "annotations": [
        {
          "corpus_id": "nlas",
          "annotation_task": [
            "Argument Type Identification"
          ],
          "description": [
            "Argument scheme validation:",
            "5 annotators validated 3810 generated argument schemes against their reported type, topic, and stance. Validation proceeded in stages, at the final stage (based on feedback from previous stages) 1917/2000 arguments generated were valid. Annotator agreement was also computed for a 10% subset of the total number of samples for both the English and Spanish examples. 4 of the 5 annotators annotated the English generated samples, while 2 of the 5 annotators annotated Spanish generated samples."
          ],
          "annotator_type": "5 professional linguists trained in argumentation theory",
          "agreement_type": "Cohen's κ",
          "agreement_score": {
            "English": 0.65,
            "Spanish": 0.22
          },
          "accessibility": "Free",
          "corpus_link": "https://zenodo.org/records/8364002",
          "corpus_name": "PARENT",
          "subset": 3810
        }
      ]
    },
    {
      "paper_id": "semeval_2016_task_6_detecting_stance_in_tweets",
      "paper_title": "SemEval-2016 Task 6: Detecting Stance in Tweets",
      "authors": [
        "Saif Mohammad",
        "Svetlana Kiritchenko",
        "Parinaz Sobhani",
        "Xiaodan Zhu",
        "Colin Cherry"
      ],
      "year": 2016,
      "paper_link": "https://aclanthology.org/S16-1003/",
      "doi": "https://doi.org/10.18653/v1/s16-1003",
      "open_alex_id": "https://openalex.org/W2460159515",
      "annotations": [
        {
          "corpus_id": "semeval_2016",
          "annotation_task": [
            "Claim Extraction with Stance Classification"
          ],
          "description": [
            "Stance detection:",
            "A dataset of 4870 English tweets is annotated with stance towards six commonly known targets in the United States (‘Atheism’, ‘Climate Change is a Real Concern’, ‘Feminist Movement’, ‘Hillary Clinton’, and ‘Legalization of Abortion’). Targets were compiled by the authors, who also compiled a list of hashtags used when tweeting about the chosen targets. These hashtags were further distinguished by whether they expressed a pro/con/ambiguous stance towards the target. The selected hashtags were used to compile a list of tweets; the hashtags were then removed as the dataset was distributed for the purposes of a shared task and classification would be simplified by their inclusion. Crowd workers from CrowdFlower were presented with tweet-target pairs and asked to label the tweets stance (with the hashtag removed), each tweet was annotated by at least 8 respondents.  For the final dataset, only instances with an agreement greater than or equal to 60% were kept. The number of tweets labelled with a neutral stance was less than 1%."
          ],
          "annotator_type": "8 crowd workers per tweet",
          "agreement_type": "Overall agreement",
          "agreement_score": 0.8185,
          "accessibility": "Free",
          "corpus_link": "https://www.saifmohammad.com/WebPages/StanceDataset.htm",
          "corpus_name": "PARENT",
          "subset": 4870
        }
      ]
    },
    {
      "paper_id": "towards_assessing_argumentation_annotation_a_first_step",
      "paper_title": "Towards Assessing Argumentation Annotation - A First Step",
      "authors": [
        "Anna Lindahl",
        "Lars Borin",
        "Jacobo Rouces"
      ],
      "year": 2019,
      "paper_link": "https://aclanthology.org/W19-4520/",
      "doi": "https://doi.org/10.18653/v1/w19-4520",
      "open_alex_id": "https://openalex.org/W2970024917",
      "annotations": [
        {
          "corpus_id": "swedish_news",
          "annotation_task": [
            "Argument Component Segmentation",
            "Argument Component Type Classification",
            "Argument Relation Identification"
          ],
          "description": [
            "Component segmentation and identification:",
            "Arguments were segmented into argument components, and their types (premises/conclusion) identified. These argument components were linked into full structures, for a downstream argument scheme annotation task. Annotation was performed using Araucaria. No predefined markers were used in the dataset to distinguish arguments, so annotators determined the boundaries of arguments themselves, because of this fuzziness agreement is measured using an equation based on the Sørensen-Dice coefficient which accounts for overlap in argument segments. Various combinations of agreement are reported in the paper, here we report the overall agreement for conclusions, for arguments where at least one premise matched (regardless of conclusion), and arguments were all premises matched (regardless of conclusion)."
          ],
          "annotator_type": "2 master’s students with solid training in linguistic analysis",
          "agreement_type": "Modified Sørensen-Dice coefficient, with a hyperparameter that is more permissive of span overlap",
          "agreement_score": {
            "Matching conclusions": 0.34,
            "At least one matching premise": 0.12,
            "All premises match": 0.03
          },
          "accessibility": "Unavailable",
          "corpus_name": "PARENT",
          "subset": 30
        },
        {
          "corpus_id": "swedish_news",
          "annotation_task": [
            "Argument Type Identification"
          ],
          "description": [
            "Argument schemes:",
            "Arguments were identified and then classified according to one of the 30 most common Walton-style argument schemes. Annotation was performed using Araucaria. Agreement over scheme is reported for arguments which match in both conclusion and premises for a previous annotation task."
          ],
          "annotator_type": "2 master’s students with solid training in linguistic analysis",
          "agreement_type": "Modified Sørensen-Dice coefficient, with a hyperparameter that is more permissive of span overlap",
          "agreement_score": {
            "Within matching conclusion": 0.22,
            "Within all arguments": 0.02
          },
          "accessibility": "Unavailable",
          "corpus_name": "PARENT",
          "subset": 30
        }
      ]
    },
    {
      "paper_id": "which_argument_is_more_convincing_analyzing_and_predicting_convincingness_of_web_arguments_using_bidirectional_lstm",
      "paper_title": "Which argument is more convincing? Analyzing and predicting convincingness of Web arguments using bidirectional LSTM",
      "authors": [
        "Ivan Habernal",
        "Iryna Gurevych"
      ],
      "year": 2016,
      "paper_link": "https://aclanthology.org/P16-1150/",
      "doi": "https://doi.org/10.18653/v1/p16-1150",
      "open_alex_id": "https://openalex.org/W2518510348",
      "annotations": [
        {
          "corpus_id": "ukp_conv_arg",
          "annotation_task": [
            "Maximal Argument Quality Assessment"
          ],
          "description": [
            "Pairwise quality assessment:",
            "Argument pairs are annotated with a label indicating which argument was more convincing. For a given argument pair A1 and A2, workers could either label A1>A2, A2>A1 or A1=A2. Each annotator also had to provide a reason for their labelling. Agreement was not reported, but several quality control measures were used for the selection of annotators. The annotated dataset was released as UKPConvArg1."
          ],
          "annotator_type": "All pairs were annotated by 5 crowd workers, in total 3900 workers participated",
          "agreement_type": "Not reported",
          "agreement_score": "Not reported",
          "accessibility": "Free",
          "corpus_link": "https://github.com/UKPLab/acl2016-convincing-arguments",
          "corpus_name": "UKPConvArg1",
          "subset": 16081
        }
      ]
    },
    {
      "paper_id": "what_makes_a_convincing_argument_empirical_analysis_and detecting_attributes_of_convincingness_in_web_argumentation",
      "paper_title": "What makes a convincing argument? Empirical analysis and detecting attributes of convincingness in Web argumentation",
      "authors": [
        "Ivan Habernal",
        "Iryna Gurevych"
      ],
      "year": 2016,
      "paper_link": "https://aclanthology.org/D16-1129/",
      "doi": "https://doi.org/10.18653/v1/d16-1129",
      "open_alex_id": "https://openalex.org/W2562522356",
      "annotations": [
        {
          "corpus_id": "ukp_conv_arg",
          "annotation_task": [
            "Other"
          ],
          "description": [
            "Reason unit classification:",
            "A bottom-up approach was used to annotate reasons given for why one argument in a pair of arguments was more convincing than another. Pairs of arguments and their reasons were obtained from a subset of 9,111 argument pairs from UKPConvArg1. From a pilot study, 19 distinct labels for reasons were obtained, for the final annotation study performed by crowd workers, these labels were decomposed into a hierarchy to be more suitable for non-experts. The annotated dataset was released as UKPConvArg2."
          ],
          "annotator_type": "All pairs were annotated by 5 crowd workers, in total 3900 workers participated",
          "agreement_type": "Not reported",
          "agreement_score": "Not reported",
          "accessibility": "Free",
          "corpus_link": "https://github.com/UKPLab/emnlp2016-empirical-convincingness",
          "corpus_name": "UKPConvArg2",
          "subset": 16081
        }
      ]
    },
    {
      "paper_id": "intertextual_correspondence_for_integrating_corpora",
      "paper_title": "Intertextual Correspondence for Integrating Corpora",
      "authors": [
        "Jacky Visser",
        "Rory Duthie",
        "John Lawrence",
        "Chris Reed"
      ],
      "year": 2018,
      "paper_link": "https://aclanthology.org/L18-1554/",
      "open_alex_id": "https://openalex.org/W2807172548",
      "annotations": [
        {
          "corpus_id": "us2016",
          "annotation_task": [
            "Argument Component Segmentation",
            "Argument Component Type Classification",
            "Argument Relation Identification",
            "Argument Relation Type Classification",
            "Argument Type Identification"
          ],
          "description": [
            "Inference anchoring theory:",
            "3 presidential debates and Reddit mega-threads that discussed them were annotated according to Inference Anchoring Theory (IAT) by 4 well-trained annotators. Annotation proceeded in stages, although sometimes later stages helped to inform a change in a previous stage, first annotators identified locutions segmenting the text into argumentative discourse units. Then annotators proceeded to identify functional transitions between locutions in the dialogue, as well as the illocutionary intention of locutions. Where relevant, annotators reconstructed the propositional content of a locution, as well as inferences between propositions. Propositions could also be related to each other by a conflict or rephrase relation. Agreement was calculated over a 10.5% subset of the debates sub-corpus annotated by 4 annotators, and a 12.6% subset of the Reddit sub-corpus was annotated by 2 annotators."
          ],
          "annotator_type": "4 unspecified annotators",
          "agreement_type": "Cohen's κ",
          "agreement_score": 0.61,
          "accessibility": "Free",
          "corpus_link": "https://corpora.aifdb.org/US2016",
          "corpus_name": "PARENT",
          "subset": 97999
        }
      ]
    },
    {
      "paper_id": "an_annotated_corpus_of_argument_schemes_in_us_election_debates",
      "paper_title": "An annotated corpus of argument schemes in US election debates",
      "authors": [
        "Jacky Visser",
        "John Lawrence",
        "Jean Wagemans",
        "Chris Reed"
      ],
      "year": 2019,
      "paper_link": "https://arg.tech/people/chris/publications/2018/issa2018.pdf",
      "open_alex_id": "https://openalex.org/W3095351355",
      "annotations": [
        {
          "corpus_id": "us2016",
          "annotation_task": [
            "Argument Component Type Classification",
            "Argument Type Identification"
          ],
          "description": [
            "Argument schemes (periodic table of arguments):",
            "3 debate transcripts (the debate sub-corpus of US2016) is annotated with a layer of argument schemes according to the Periodic Table of Arguments (PTA). Annotation according to the PTA involves 3 subtasks to identify 3 distinguishing characteristics of arguments, which re-use previous Inference Anchoring Theory annotations from the original US2016 corpus. The first annotation task involves distinguishing first-order and second-order arguments, which is a classification over the inference relation between two propositions. An inference relation is first-order if it connects two propositions which both contain a subject-predicate pair, otherwise the inference relation is second order if its premise is a locution. The second annotation task involves distinguishing predicate and subject arguments, where an inference relation is classified as a predicate argument if the propositions involved share the same subject term to which different predicates are applied and is a subject argument in the opposite case. Finally, propositions were also annotated as either fact, value or policy. Annotation was performed by 2 annotators, and agreement was calculated over a 10% sample of the corpus annotated by both annotators."
          ],
          "annotator_type": "2 unspecified annotators",
          "agreement_type": "Cohen's κ",
          "agreement_score": {
            "First/second-order arguments": 0.658,
            "Predicate/subject arguments": 0.851,
            "Fact/value/policy": 0.778
          },
          "accessibility": "Free",
          "corpus_link": "https://corpora.aifdb.org/US2016G1tvWAGEMANS",
          "corpus_name": "US2016G1tvWAGEMANS",
          "subset": "See description"
        }
      ]
    },
    {
      "paper_id": "yes_we_can_mining_arguments_in_50_years_of_us_presidential_campaign_debates",
      "paper_title": "Yes, we can! Mining Arguments in 50 Years of US Presidential Campaign Debates",
      "authors": [
        "Shohreh Haddadan",
        "Elena Cabrio",
        "Serena Villata"
      ],
      "year": 2019,
      "paper_link": "https://aclanthology.org/P19-1463/",
      "doi": "https://doi.org/10.18653/v1/p19-1463",
      "open_alex_id": "https://openalex.org/W2949928613",
      "annotations": [
        {
          "corpus_id": "us_elec_deb_60_to_16",
          "annotation_task": [
            "Argument Component Segmentation"
          ],
          "description": [
            "Argument component segmentation:",
            "39 political debate transcripts are segmented in argumentative and non-argumentative text by 3 annotators. Each transcript was independently annotated by at least 2 annotators, and 19 debates were annotated by all 3 annotators to compute agreement."
          ],
          "annotator_type": "3 annotators, 2 expert annotators were also used in a reconciliation phase",
          "agreement_type": "Fleiss' κ",
          "agreement_score": 0.57,
          "accessibility": "Free",
          "corpus_link": "https://github.com/ElecDeb60To16/Dataset",
          "corpus_name": "PARENT",
          "subset": 39
        },
        {
          "corpus_id": "us_elec_deb_60_to_16",
          "annotation_task": [
            "Argument Component Type Classification"
          ],
          "description": [
            "Argument component type classification:",
            "Argumentative and non-argumentative segments of text in 39 political debate transcripts are annotated by their type: either \"claim\" or \"premise\". Each transcript was independently annotated by at least 2 annotators, and 19 debates were annotated by all 3 annotators to compute agreement."
          ],
          "annotator_type": "3 annotators, 2 expert annotators were also used in a reconciliation phase",
          "agreement_type": "Fleiss' κ",
          "agreement_score": 0.4,
          "accessibility": "Free",
          "corpus_link": "https://github.com/ElecDeb60To16/Dataset",
          "corpus_name": "PARENT",
          "subset": 39
        }
      ]
    },
    {
      "paper_id": "fallacious_argument_classification_in_political_debates",
      "paper_title": "Fallacious Argument Classification in Political Debates",
      "authors": [
        "Pierpaolo Goffredo",
        "Shohreh Haddadan",
        "Vorakit Vorakitphan",
        "Elena Cabrio",
        "Serena Villata"
      ],
      "year": 2022,
      "paper_link": "https://www.ijcai.org/proceedings/2022/575",
      "doi": "https://doi.org/10.24963/ijcai.2022/575",
      "open_alex_id": "https://openalex.org/W4285602624",
      "annotations": [
        {
          "corpus_id": "us_elec_deb_60_to_16",
          "annotation_task": [
            "Minimal Argument Quality Assessment"
          ],
          "description": [
            "Fallacy detection:",
            "31 political debates are annotated at the span level with 6 types of fallacies (ad hominem, appeal to emotion, appeal to authority, slippery slope, false cause, and slogan) which were selected based on an exploratory study on the dataset by an expert. Annotators selected spans which they determined contained a fallacy, of the 6 possible fallacies 4 were present in the final dataset which consisted of 1628 annotated fallacious arguments. Agreement was calculated based on the 3 annotators annotation of 9 sections from 5 debates."
          ],
          "annotator_type": "3 annotators with a background in computational linguistics",
          "agreement_type": "Krippendorff's α",
          "agreement_score": {
            "Ad Hominem": 0.5315,
            "Appeal to Authority": 0.5806,
            "Appeal to Emotion": 0.4604,
            "Slogans": 0.5995
          },
          "accessibility": "Free",
          "corpus_link": "https://github.com/pierpaologoffredo/IJCAI2022",
          "corpus_name": "USElecDeb60To16-fallacy",
          "subset": 31
        }
      ]
    },
    {
      "paper_id": "argumentation_mining_in_user_generated_web_discourse",
      "paper_title": "Argumentation Mining in User-Generated Web Discourse",
      "authors": [
        "Ivan Habernal",
        "Iryna Gurevych"
      ],
      "year": 2017,
      "paper_link": "https://aclanthology.org/J17-1004/",
      "doi": "https://doi.org/10.1162/coli_a_00276",
      "open_alex_id": "https://openalex.org/W2236647290",
      "annotations": [
        {
          "corpus_id": "web_discourse",
          "annotation_task": [
            "Argument Component Segmentation",
            "Argument Component Type Classification"
          ],
          "description": [
            "Toulmin component classification:",
            "5444 documents containing web-based discourse on controversial documents were annotated at the span level with their argumentative components. These argument components were derived from Toulmin’s model, and annotators could choose to label spans as either claim, premise, backing, rebuttal, and refutation. Annotation was performed in stages, with discussions around problematic cases occurring early leading to a refinement of annotation guidelines. Annotators determined spans themselves (argument segmentation) and the gold label was obtained by majority vote on overlapping spans, and problematic cases resolved through further discussion. Agreement was reported using Krippendorf’s α. Agreement varied significantly on both the topic discussed in the document and the type of the document. A joint agreement across all types of documents, all topics, and all kinds of argumentative components was reported, which we list here."
          ],
          "annotator_type": "All documents were annotated by 3 independent annotators who participated in two training sessions",
          "agreement_type": "Krippendorff's α",
          "agreement_score": 0.48,
          "accessibility": "Free",
          "corpus_link": "https://tudatalib.ulb.tu-darmstadt.de/items/d4a7ac0c-e7a8-466a-855a-917e42a24342",
          "corpus_name": "PARENT",
          "subset": 5444
        }
      ]
    },
    {
      "paper_id": "a_news_editorial_corpus_for_mining_argumentation_atrategies",
      "paper_title": "A News Editorial Corpus for Mining Argumentation Strategies",
      "authors": [
        "Khalid Al-Khatib",
        "Henning Wachsmuth",
        "Johannes Kiesel",
        "Matthias Hagen",
        "Benno Stein"
      ],
      "year": 2016,
      "paper_link": "https://aclanthology.org/C16-1324/",
      "open_alex_id": "https://openalex.org/W2579772177",
      "annotations": [
        {
          "corpus_id": "webis",
          "annotation_task": [
            "Argument Component Type Classification"
          ],
          "description": [
            "Argumentative discourse unit classification:",
            "300 news editorials are annotated at the span level with their argumentative discourse unit. Each editorial was annotated by 3 annotators (out of 4 total annotators), and text was pre-segmented into text spans (annotators did not determine spans themselves). Annotators could label spans as one of 6 possible argumentative discourse unit types (common ground, assumption, anecdote, testimony, statistics, or other), they could also label spans as having no type, or as a continuation of the previous span."
          ],
          "annotator_type": "4 crowd annotators from upwork.com",
          "agreement_type": "Fleiss' κ",
          "agreement_score": {
            "Common ground": 0.114,
            "Assumption": 0.613,
            "Anecdote": 0.399,
            "Testimony": 0.591,
            "Statistics": 0.582,
            "Other": 0.152,
            "No unit": 0.365,
            "Continued": 0.684,
            "Overall": 0.56
          },
          "accessibility": "Free",
          "corpus_link": "https://webis.de/data/webis-editorials-16.html",
          "corpus_name": "PARENT",
          "subset": 300
        }
      ]
    },
    {
      "paper_id": "arguments_about_deletion_how_experience_improves_the_acceptability_of_arguments_in_ad_hoc_online_task_groups",
      "paper_title": "Arguments about deletion: how experience improves the acceptability of arguments in ad-hoc online task groups",
      "authors": [
        "Jodi Schneider",
        "Krystian Samp",
        "Alexandre Passant",
        "Stefan Decker"
      ],
      "year": 2013,
      "paper_link": "https://dl.acm.org/doi/10.1145/2441776.2441897",
      "doi": "https://doi.org/10.1145/2441776.2441897",
      "annotations": [
        {
          "corpus_id": "wiki_deletion",
          "annotation_task": [
            "Argument Type Identification"
          ],
          "description": [
            "Argument scheme identification:",
            "741 comments on Wikipedia debate/discussion pages were annotated with their argument schemes (as well as labels no reason was given or no argument present). Annotation proceeded in several rounds, in the second round the overall prevalence of various argument schemes was determined, this was used to guide annotation in a third and fourth round. In the fourth and final round two annotators annotated the corpus, agreement was reported for this fourth and final round of annotation."
          ],
          "annotator_type": "2 student annotators (one graduate and one undergraduate)",
          "agreement_type": "Cohen's κ",
          "agreement_score": 0.48,
          "accessibility": "Unavailable",
          "corpus_name": "PARENT",
          "subset": 741
        }
      ]
    }
  ]
}
