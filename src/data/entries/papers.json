{
  "papers": [
    {
      "paper_id": "an_annotated_corpus_of_argumentative_microtexts",
      "paper_title": "An annotated corpus of argumentative microtexts",
      "authors": ["Andreas Peldzus", "Manfred Stede"],
      "year": 2016,
      "paper_link": "https://www.ling.uni-potsdam.de/~peldszus/eca2015-preprint.pdf",
      "annotations": [
        {
          "corpus_id": "microtexts_v1",
          "annotation_task": [
            "Argument Component Segmentation",
            "Argument Component Type Classification",
            "Argumentative Relation Identification",
            "Argumentative Relation Type Classification"
          ],
          "description": [
            "Texts are segmented into Argumentative Discourse Units (ADUs) with support and attack links."
          ],
          "annotator_count": 3,
          "annotator_type": ["Expert", "Other"],
          "agreement_type": "Fleiss' Kappa",
          "agreement_score": 0.83,
          "accessibility": "Free",
          "corpus_link": "https://github.com/peldszus/arg-microtexts"
        }
      ]
    },
    {
      "paper_id": "a_news_editorial_corpus_for_argumentation_mining_strategies",
      "paper_title": "A news editorial corpus for argumentation mining strategies",
      "authors": [
        "Khalid Al-Khatib",
        "Henning Wachsmuth",
        "Johannes Kiesel",
        "Matthias Hagen",
        "Benno Stein"
      ],
      "year": 2016,
      "paper_link": "https://aclanthology.org/C16-1324/",
      "open_alex_id": "https://openalex.org/W2579772177",
      "annotations": [
        {
          "corpus_id": "webis_editorials_16",
          "annotation_task": [
            "Argument Component Segmentation",
            "Argument Component Type Classification"
          ],
          "description": [
            "Each ADU is labeled as one of six types: Assumption (general opinions), Anecdote (stories or examples), Testimony (expert or personal statements), Statistics (numerical data), Common Ground (shared beliefs), or Other (miscellaneous).",
            "Three professional annotators from upwork.com, trained for the task, labeled these units"
          ],
          "annotator_count": 4,
          "annotator_type": ["Expert"],
          "agreement_type": "Fleiss' Kappa",
          "agreement_score": 0.56,
          "accessibility": "Free",
          "corpus_link": "https://webis.de/data/webis-editorials-16.html"
        }
      ]
    },
    {
      "paper_id": "annotating_agreement_and_disagreement_in_threaded_discussion",
      "paper_title": "Annotating agreement and disagreement in threaded discussion",
      "authors": ["Jacob Andreas", "Sara Rosenthal", "Kathy McKeown"],
      "year": 2012,
      "paper_link": "https://aclanthology.org/L12-1650/",
      "open_alex_id": "https://openalex.org/W2251588411",
      "annotations": [
        {
          "corpus_id": "agreement_in_wikipedia_talk_pages",
          "annotation_task": [
            "Argumentative Relation Type Classification",
            "Other"
          ],
          "description": [
            "Annotations target ordered sentence pairs, consisting of an antecedent (the quoted statement) and a reaction (the response), identified within explicit reply edges in the discussion thread. Each Q-R pair is labeled for Type (agreement, disagreement, or none) and Mode (direct response, direct paraphrase, indirect response, indirect paraphrase).",
            "The corpus was annotated by three trained undergraduate students, ensuring consistency through structured training and the use of a contextual annotation tool."
          ],
          "annotator_count": 3,
          "annotator_type": ["Student"],
          "agreement_type": "Cohen's Kappa",
          "agreement_score": 0.66,
          "accessibility": "Free",
          "corpus_link": "https://www.cs.columbia.edu/~sara/data.php"
        },
        {
          "corpus_id": "agreement_in_wikipedia_talk_pages",
          "annotation_task": ["Argumentative Relation Type Classification"],
          "description": [
            "Annotations target ordered sentence pairs, consisting of an antecedent (the quoted statement) and a reaction (the response), identified within explicit reply edges in the discussion thread. Each Q-R pair is labeled for Type (agreement, disagreement, or none).",
            "The corpus was annotated by three trained undergraduate students, ensuring consistency through structured training and the use of a contextual annotation tool."
          ],
          "annotator_count": 3,
          "annotator_type": ["Student"],
          "agreement_type": "Cohen's Kappa",
          "agreement_score": 0.73,
          "accessibility": "Free",
          "corpus_link": "https://www.cs.columbia.edu/~sara/data.php"
        }
      ]
    },
    {
      "paper_id": "backup_your_stance",
      "paper_title": "Back up your stance: Recognizing arguments in online discussions",
      "authors": ["Filip Boltužić", "Jan Šnajder"],
      "year": 2014,
      "paper_link": "https://aclanthology.org/W14-2107/",
      "open_alex_id": "https://openalex.org/W2250397934",
      "doi": "https://doi.org/10.3115/v1/w14-2107",
      "annotations": [
        {
          "corpus_id": "comarg",
          "annotation_task": ["Argumentative Relation Type Classification"],
          "description": [
            "Each comment-argument pair is labeled on a five-point ordinal scale to indicate the comment’s stance toward the argument: A: Explicit Attack (clearly opposes the argument). a: Implicit/Vague Attack (subtly opposes the argument). N: No Use (does not reference the argument). s: Implicit/Vague Support (subtly supports the argument). S: Explicit Support (clearly supports the argument).",
            "NOTE: Paper reports multiple types of agreement, we only note Fleiss' Kappa here."
          ],
          "annotator_count": 3,
          "annotator_type": ["Expert"],
          "agreement_type": "Fleiss' Kappa",
          "agreement_score": 0.49,
          "accessibility": "Free",
          "corpus_link": "https://takelab.fer.hr/data/comarg/"
        }
      ]
    },
    {
      "paper_id": "analyzing_argumentative_discourse_units_in_online_interactions",
      "paper_title": "Analyzing argumentative discourse units in online interactions",
      "authors": [
        "Debanjan Ghosh",
        "Smaranda Muresan",
        "Nina Wacholder",
        "Mark Aakhus",
        "Matthew Mitsui"
      ],
      "year": 2014,
      "paper_link": "https://aclanthology.org/W14-2106/",
      "open_alex_id": "https://openalex.org/W2251521080",
      "annotations": [
        {
          "corpus_id": "technical_blogs",
          "annotation_task": [
            "Argument Component Segmentation",
            "Argument Component Type Classification",
            "Argumentative Relation Identification"
          ],
          "description": [
            "The annotation scheme is based on Pragmatic Argumentation Theory (PAT), which frames arguments as interactions between Callouts and Targets. A Callout is a subsequent action that expresses an explicit stance or rationale toward a Target, defined as a prior action in the discussion. Five graduate students with strong humanities backgrounds were trained as annotators to perform an annotation study that followed a three-step process: (1) segmenting the text into Argumentative Discourse Units (ADUs), (2) classifying each segment as a Callout or Target, and (3) linking each Callout to its most recent Target. This coarse-grained approach captures the structure of argumentative interactions in online discussions.",
            "NOTE: Agreement is reported seperately for each topic in the corpus. The agreement reported below is agreement across the whole annotation task (segementation and type classification). See the paper for more precise details on IAA."
          ],
          "annotator_count": 5,
          "annotator_type": ["Expert", "Student"],
          "agreement_type": "Krippendorff's Alpha",
          "agreement_score": {
            "Android": 0.64,
            "iPad": 0.73,
            "Layoffs": 0.87,
            "Twitter": 0.82
          },
          "accessibility": "Free",
          "corpus_link": "https://salts.rutgers.edu/identifying-the-language-of-opposition-in-online-interactions/"
        },
        {
          "corpus_id": "technical_blogs",
          "annotation_task": ["Argumentative Relation Type Classification"],
          "description": [
            "The annotation scheme is based on Pragmatic Argumentation Theory (PAT), which frames arguments as interactions between Callouts and Targets. A Callout is a subsequent action that expresses an explicit stance or rationale toward a Target, defined as a prior action in the discussion. Five graduate students with strong humanities backgrounds were trained as annotators to perform the annotation.",
            "In this task, annotators had to perform a fine-grained classification of relations between callouts and targets, as either agree, disagree, or other.",
            "NOTE: Only a range of Fleiss' Kappa values was reports in the paper for this task."
          ],
          "annotator_count": 5,
          "annotator_type": ["Crowd"],
          "agreement_type": "Fleiss' Kappa",
          "agreement_score": [0.45, 0.55],
          "accessibility": "Free",
          "corpus_link": "https://salts.rutgers.edu/identifying-the-language-of-opposition-in-online-interactions/"
        },
        {
          "corpus_id": "technical_blogs",
          "annotation_task": ["Claim Extraction with Stance Classification"],
          "description": [
            "The annotation scheme is based on Pragmatic Argumentation Theory (PAT), which frames arguments as interactions between Callouts and Targets. A Callout is a subsequent action that expresses an explicit stance or rationale toward a Target, defined as a prior action in the discussion. Five graduate students with strong humanities backgrounds were trained as annotators to perform the annotation.",
            "In this task, annotators had to identify the stance and rationale of callouts towards their targets, including the exact boundaries of the text segments.",
            "NOTE: No agreement reported, although for 50% of callouts the stance and rationale were easily identified by the annotators."
          ],
          "annotator_count": 5,
          "annotator_type": ["Crowd"],
          "agreement_type": "Other",
          "agreement_score": "?",
          "accessibility": "Free",
          "corpus_link": "https://salts.rutgers.edu/identifying-the-language-of-opposition-in-online-interactions/"
        }
      ]
    },
    {
      "paper_id": "argumentation_mining_in_usergenerated_webdiscourse",
      "paper_title": "Argumentation mining in user-generated web discourse",
      "authors": ["Ivan Habernal", "Iryna Gurevych"],
      "year": 2017,
      "paper_link": "https://direct.mit.edu/coli/article/43/1/125/1561/Argumentation-Mining-in-User-Generated-Web",
      "doi": "https://doi.org/10.1162/coli_a_00276",
      "open_alex_id": "https://openalex.org/W3105663928",
      "annotations": [
        {
          "corpus_id": "amugwd_unfiltered",
          "annotation_task": ["Other"],
          "description": [
            "Three near-native annotators labeled 990 comments and forum posts as persuasive or non-persuasive over 15 hours each, identifying 524 persuasive documents for further analysis."
          ],
          "annotator_count": 3,
          "annotator_type": ["Other"],
          "agreement_type": "Fleiss' Kappa",
          "agreement_score": 0.59,
          "accessibility": "Free",
          "corpus_link": "https://tudatalib.ulb.tu-darmstadt.de/items/d4a7ac0c-e7a8-466a-855a-917e42a24342"
        },
        {
          "corpus_id": "amugwd_filtered",
          "annotation_task": [
            "Argument Component Segmentation",
            "Argument Component Type Classification"
          ],
          "description": [
            "340 persuasive documents were annotated using a modified Toulmin model, focusing on the logos dimension (claims, premises, backing, rebuttal, refutation). Sequence labeling with BIO encoding (11 classes, e.g., Claim-B, Premise-I) was applied at the token level, defaulting to sentence-level units unless finer granularity was needed.",
            "NOTE: Individual IAA is reported for each component, here we report the overall Krippendorff's Alpha reported in the paper."
          ],
          "annotator_count": 3,
          "annotator_type": ["Other"],
          "agreement_type": "Krippendorff's Alpha",
          "agreement_score": 0.48,
          "accessibility": "Free",
          "corpus_link": "https://tudatalib.ulb.tu-darmstadt.de/items/d4a7ac0c-e7a8-466a-855a-917e42a24342"
        }
      ]
    },
    {
      "paper_id": "internet_argument_corpus2",
      "paper_title": "The Internet Argument Corpus 2.0: An SQL schema for dialogic social media and the corpora to go with it",
      "authors": [
        "Rob Abbott",
        "Brian Ecker",
        "Pranav Anand",
        "Marilyn A. Walker"
      ],
      "year": 2016,
      "paper_link": "https://aclanthology.org/L16-1704/",
      "open_alex_id": "https://openalex.org/W2577656483",
      "annotations": [
        {
          "corpus_id": "internet_argument_corpus_v2_4forums",
          "annotation_task": [
            "Claim Extraction with Stance Classification",
            "Other"
          ],
          "description": [
            "Annotating posts from 4forums.com with topic, author stance, agreement, sarcasm, and hostility.",
            "NOTE: No agreement reported."
          ],
          "annotator_count": "?",
          "annotator_type": ["Crowd"],
          "agreement_type": "Other",
          "agreement_score": "?",
          "accessibility": "Free",
          "corpus_link": "https://nlds.soe.ucsc.edu/software"
        },
        {
          "corpus_id": "internet_argument_corpus_v2_convinceme",
          "annotation_task": [
            "Claim Extraction with Stance Classification",
            "Other"
          ],
          "description": [
            "Annotating dialogues from ConvinceMe.net (a structured debate website) with topic, discussion stance, and broader topic stance.",
            "NOTE: No agreement reported."
          ],
          "annotator_count": "?",
          "annotator_type": ["Crowd"],
          "agreement_type": "Other",
          "agreement_score": "?",
          "accessibility": "Free",
          "corpus_link": "https://nlds.soe.ucsc.edu/software"
        },
        {
          "corpus_id": "internet_argument_corpus_v2_createdebate",
          "annotation_task": [
            "Claim Extraction with Stance Classification",
            "Other"
          ],
          "description": [
            "Gun control debates from CreateDebate.com, the site forces users to specify their stance, and a label (support, clarify, dispute). It also allows users to vote on posts, which can be used to examine argument effectiveness.",
            "NOTE: No annotation reported, as the annotations are natural."
          ],
          "annotator_count": 0,
          "annotator_type": ["Other"],
          "agreement_type": "Other",
          "agreement_score": "?",
          "accessibility": "Free",
          "corpus_link": "https://nlds.soe.ucsc.edu/software"
        }
		]
    },
	{
	  "paper_id": "ethix_a_dataset_for_argument_scheme_classification",
      "paper_title": "EthiX: A Dataset for Argument Scheme Classification in Ethical Debates",
      "authors": [
        "Elfia Bezou-Vrakatseli",
        "Oana Cocarascu",
        "Sanjay Modgil"
      ],
      "year": 2025,
      "paper_link": "https://ebooks.iospress.nl/DOI/10.3233/FAIA240919",
      "open_alex_id": "https://openalex.org/W4403487202",
      "doi": "https://doi.org/10.3233/faia240919",
      "annotations": [
        {
          "corpus_id": "ethix",
          "annotation_task": [
            "Argument Type Identification"
          ],
          "description": [
            "Argument scheme identification:",
            "arguments are annotated arguments with their argument scheme according to Walton's taxonomy (e.g., argument from example, values, positive/negative consequences, cause-to-effect, expert opinion, alternatives, analogy), with Argument Scheme Key (ASK) guidelines to aid consistent labeling. Annotation was performed by the authors, and agreement measured with another PhD student who was not an expert in argumentation."
          ],
          "annotator_count": 4,
          "annotator_type": ["Expert", "Student"],
          "agreement_type": "Cohen's Kappa",
          "agreement_score": 0.523,
          "accessibility": "Free",
          "corpus_link": "https://github.com/ElfiaBv/EthiX"
		}
		]
	},
	{
	  "paper_id": "annotating_argument_components_and_relations_in_persuasive_essays",
      "paper_title": "Annotating Argument Components and Relations in Persuasive Essays",
      "authors": [
        "Christian Stab",
        "Iryna Gurevych"
      ],
      "year": 2014,
      "paper_link": "https://aclanthology.org/C14-1142/",
      "open_alex_id": "https://openalex.org/W2251931307",
      "annotations": [
        {
          "corpus_id": "aae_v1",
          "annotation_task": [
            "Argument Component Segmentation",
			"Argument Component Type Classification"
          ],
          "description": [
            "Argument component segmentation and classification: essays are segmented into their argumentative components. Annotators first identified the major claim of the essay, and then the claims and premises in each paragraph. Because there are no predefined markables in the data, annotators had to determine the boundaries of argument components themselves, performing a segmentation of the argumentative text. A modification of Krippendorf’s Alpha that allows for disagreement in span boundaries was used to capture annotation performance in both the segmentation of argumentative units and their classification."
          ],
          "annotator_count": 3,
          "annotator_type": ["Other"],
          "agreement_type": "Other",
          "agreement_score": {
			"Major Claim": 0.7726, "Claim": 0.6033, "Premise": 0.7594
		  },
          "accessibility": "Free",
          "corpus_link": "https://tudatalib.ulb.tu-darmstadt.de/items/a52f7fef-4388-420c-91f0-ac03dac26bbb"
		},
		{
		  "corpus_id": "aae_v1",
		  "annotation_task": [
		    "Argumentative Relation Type Classification"
		  ],
		  "description": [
		    "Argument component relation annotation:",
			"essays are segmented into their argumentative components which are then classified. Annotators first identified the major claim of the essay, and then the claims and premises in each paragraph."
		  ],
		  "annotator_count": 3,
		  "annotator_type": ["Other"],
		  "agreement_type": "Krippendorff's Alpha",
		  "agreement_score": {
		    "Support": 0.8120, "Attack": 0.8066
		  },
		  "accessibility": "Free",
		  "corpus_link": "https://tudatalib.ulb.tu-darmstadt.de/items/a52f7fef-4388-420c-91f0-ac03dac26bbb"
		},
		{
		  "corpus_id": "aae_v1",
		  "annotation_task": [
		    "Argument Type Identification"
		  ],
		  "description": [
		    "Argumentum Model of Topics:",
			"a subset of 30 essays from AAE1 were annotated using an annotation scheme based on the Argumentum Model of Topics for a pilot study. Several student annotators were given a set of identification questions for a group of argument schemes from the Argumentum Model of Topics, and asked to identify whichquestion most closely matches argumentative components linked by a support relation (annotators could select no argument)."
		  ],
		  "annotator_count": 9,
		  "annotator_type": ["Student"],
		  "agreement_type": "Fleiss' Kappa",
		  "agreement_score": 0.1,
		  "accessibility": "Unavailable",
		  "corpus_link": "https://aclanthology.org/W16-2810.pdf"
		}
		]
	},
	{
	  "paper_id": "towards_assessing_argument_annotation_-_a_first_step",
      "paper_title": "Towards Assessing Argument Annotation - A First Step",
      "authors": [
        "Anna Lindahl",
        "Lars Borin",
		"Jacobo Rouces Gonzalez"
      ],
      "year": 2019,
      "paper_link": "https://aclanthology.org/W19-4520/",
      "open_alex_id": "https://openalex.org/W2970024917",
      "annotations": [
        {
          "corpus_id": "swedish_news_editorials",
          "annotation_task": [
            "Argument Component Segmentation",
			"Argument Component Type Classification",
			"Argument Relation Identification"
          ],
          "description": [
		    "Component segmentation and identification:",
            "arguments were segmented into argument components, and their types (premises/conclusion) identified. These argument components were linked into full structures, for a downstream argument scheme annotation task. Annotation was performed using Araucaria. No predefined markers were used in the dataset to distinguish arguments, so annotators determined the boundaries of arguments themselves, because of this fuzziness agreement is measured using an equation based on the Sørensen-Dice coefficient which accounts for overlap in argument segments. Various combinations of agreement are reported in the paper, here we report the overall agreement for conclusions, for arguments where at least one premise matched (regardless of conclusion), and arguments were all premises matched (regardless of conclusion)."
          ],
          "annotator_count": 2,
          "annotator_type": ["Student"],
          "agreement_type": "Other",
          "agreement_score": {
			"Matching_conclusions": 0.34, "At_least_one_matching_premise": 0.12, "All premises match": 0.03,
		  },
          "accessibility": "Unavailable"
		}
	]
	},
	{
	  "paper_id": "which_argument_is_more_convincing?_analyzing_and_predicting_convincingness_of_web_arguments_using_bidirectional_lstm",
      "paper_title": "Which argument is more convincing? Analyzing and predicting convincingness of Web arguments using bidirectional LSTM",
      "authors": [
        "Ivan Habernal",
		"Iryna Gurevych"
      ],
      "year": 2016,
      "paper_link": "https://aclanthology.org/P16-1150/",
      "open_alex_id": "https://openalex.org/W2518510348",
      "annotations": [
        {
          "corpus_id": "UKPConvArg",
          "annotation_task": [
            "Maximal Argument Quality Assessment"
          ],
          "description": [
		    "Pairwise quality assessment:",
            "argument pairs are annotated with a label indicating which argument was more convincing. For a given argument pair A1 and A2, workers could either label A1>A2, A2>A1 or A1=A2. Each annotator also had to provide a reason for their labelling. Agreement was not reported, but several quality control measures were used for the selection of annotators. The annotated dataset was released as UKPConvArg1."
          ],
          "annotator_count": 5,
          "annotator_type": ["Crowd"],
          "agreement_type": "other",
          "agreement_score": {
			"?"
			},
          "accessibility": "Free",
		  "corpus_link": "https://github.com/UKPLab/acl2016-convincing-arguments"
		},
		{
		  "corpus_id": "UKPConvArg",
		  "annotation_task": [
		    "Other"
		  ],
		  "description": [
		    "Reason unit classification:",
			"a bottom-up approach was used to annotate reasons given for why one argument in a pair of arguments was more convincing than another. Pairs of arguments and their reasons were obtained from a subset of 9,111 argument pairs from UKPConvArg1. From a pilot study, 19 distinct labels for reasons were obtained, for the final annotation study performed by crowd workers, these labels were decomposed into a hierarchy to be more suitable for non-experts. The annotated dataset was released as UKPConvArg2."
		  ],
		  "annotator_count": 5,
		  "annotator_type": ["Crowd"],
		  "agreement_type": "Other",
		  "agreement_score": {"?"},
		  "accessibility": "Free",
		  "corpus_link": "https://github.com/UKPLab/emnlp2016-empirical-convincingness"
		}
		]
	},
	{
	  "paper_id": "argumentation_mining_in_usergenerated_webdiscourse",
      "paper_title": "Argumentation Mining in User-Generated Web Discourse",
      "authors": [
        "Ivan Habernal",
		"Iryna Gurevych"
      ],
      "year": 2017,
      "paper_link": ": https://aclanthology.org/J17-1004/",
      "open_alex_id": "https://openalex.org/W2236647290",
      "annotations": [
        {
          "corpus_id": "the_web_discourse_corpus",
          "annotation_task": [
            "Argument Component Segmentation",
			"Argument Component Type Classification"
          ],
          "description": [
		    "Toulmin component classification:",
			"5444 documents containing web-based discourse on controversial documents were annotated at the span level with their argumentative components. These argument components were derived from Toulmin’s model, and annotators could choose to label spans as either claim, premise, backing, rebuttal, and refutation. Annotation was performed in stages, with discussions around problematic cases occurring early leading to a refinement of annotation guidelines. Annotators determined spans themselves (argument segmentation) and the gold label was obtained by majority vote on overlapping spans, and problematic cases resolved through further discussion. Agreement was reported using Krippendorf’s Alpha. Agreement varied significantly on both the topic discussed in the document and the type of the document. A joint agreement across all types of documents, all topics, and all kinds of argumentative components was reported, which we list here.",
            ],
          "annotator_count": 3,
          "annotator_type": ["Other"],
          "agreement_type": "Krippendorff's Alpha",
          "agreement_score": 0.48,
          "accessibility": "https://tudatalib.ulb.tu-darmstadt.de/items/d4a7ac0c-e7a8-466a-855a-917e42a24342"
		}
	]
	}
]
}