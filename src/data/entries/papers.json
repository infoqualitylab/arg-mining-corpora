{
  "papers": [
     {
      "paper_id": "aeg_argumentative_essay_generation_via_a_dual_decoder_model_with_content_planning",
      "paper_title": "AEG: Argumentative Essay Generation via A Dual-Decoder Model with Content Planning",
      "authors": ["Jianzhu Bao", "Yasheng Wang", "Yitong Li", "Fei Mi", "Ruifeng X"],
      "year": 2022,
      "paper_link": "https://aclanthology.org/2022.emnlp-main.343/",
	  "doi": "https://doi.org/10.18653/v1/2022.emnlp-main.343",
	  "open_alex_id": "https://openalex.org/W4385573259",
      "annotations": [
        {
          "corpus_id": "arg_essay",
          "annotation_task": [
            "Maximal Argument Quality Assessment"
          ],
          "description": [
			"Argument quality assessment:"
            "A dataset of prompt-essay pairs is used to evaluate the ability of several computational models in their performance of automatic essay generation. Part of the evaluation was performed human annotators, who gave scores from 1-5 (worst to best) for generated essays from 50 different prompts on quality dimensions of relevance, coherence, and content richness. No annotator agreement was reported, this human annotation is not publicly available, but the full ArgEssay dataset was made available."
          ],
          "annotator_type": "3 unspecified annotators",
          "agreement_type": "Not reported",
          "agreement_score": "Not reported",
          "accessibility": "Free",
          "corpus_link": "https://github.com/HITSZ-HLT/AEG"
        }
      ]
    },
	{
      "paper_id": "annotating_argument_components_and_relations_in_persuasive_essays",
      "paper_title": "Annotating Argument Components and Relations in Persuasive Essays",
      "authors": ["Christian Stab", "Iryna Gurevych"],
      "year": 2014,
      "paper_link": "https://aclanthology.org/C14-1142/",
	  "open_alex_id": "https://openalex.org/W2251931307",
      "annotations": [
        {
          "corpus_id": "aae_v1",
          "annotation_task": [
            "Argument Component Segmentation", "Argument Component Type Classification"
          ],
          "description": [
		  "Argument component segmentation and classification:",
		  "Essays are segmented into their argumentative components. Annotators first identified the major claim of the essay, and then the claims and premises in each paragraph. Because there are no predefined markables in the data, annotators had to determine the boundaries of argument components themselves, performing a segmentation of the argumentative text. A modification of Krippendorf’s α that allows for disagreement in span boundaries was used to capture annotation performance in both the segmentation of argumentative units and their classification."
          ],
          "annotator_type": "3 non-expert annotators",
          "agreement_type": "Krippendorff's αU",
          "agreement_score": {"Major Claim":  0.7726, "Claim":  0.6033, "Premise":  0.7594},
          "accessibility": "Free",
          "corpus_link": "https://tudatalib.ulb.tu-darmstadt.de/items/a52f7fef-4388-420c-91f0-ac03dac26bbb"
        },
		{
          "corpus_id": "aae_v1",
          "annotation_task": [
            "Argument Relation Type Classification"
          ],
          "description": [
		  "Argument component relation annotation:",
		  "Classified argument components (premises, claims, and major claims) are linked by either support or attack relations. Annotators first linked claims and premises within each paragraph, before linking claims to major claims with either a support or attack relation."
          ],
          "annotator_type": "3 non-expert annotators",
          "agreement_type": "Krippendorff's α",
          "agreement_score": {"Support": 0.8120, "Attack": 0.8066},
          "accessibility": "Free",
          "corpus_link": "https://tudatalib.ulb.tu-darmstadt.de/items/a52f7fef-4388-420c-91f0-ac03dac26bbb"
        }
      ]
    },
	{
      "paper_id": "towards_feasible_guidelines_for_the_annotation_of_argument_schemes",
      "paper_title": "Towards Feasible Guidelines for the Annotation of Argument Schemes",
      "authors": ["Elena Musi", "Debanjan Ghosh", "Smaranda Muresan"],
      "year": 2016,
      "paper_link": "https://aclanthology.org/W16-2810/",
	  "https://doi.org/10.18653/v1/w16-2810",
	  "open_alex_id": "https://openalex.org/W2508604498",
      "annotations": [
        {
          "corpus_id": "aae_v1",
          "annotation_task": [
            "Argument Type Identification"
          ],
          "description": [
		  "Argumentum Model of Topics:",
		  "A subset of 30 essays from AAE1 were annotated using an annotation scheme based on the Argumentum Model of Topics for a pilot study. Several student annotators were given a set of identification questions for a group of argument schemes from the Argumentum Model of Topics, and asked to identify which question most closely matches argumentative components linked by a support relation (annotators could select no argument)."
          ],
          "annotator_type": "9 graduate students with no specific background in linguistics or argumentation",
          "agreement_type": "Fleiss’ κ",
          "agreement_score": 0.1,
          "accessibility": "Unavailable",
          "corpus_link": "Unavailable"
        }
      ]
    },
	{
      "paper_id": "parsing_argumentation_structures_in_persuasive_essays",
      "paper_title": "Parsing Argumentation Structures in Persuasive Essays",
      "authors": ["Christian Stab", "Iryna Gurevych"],
      "year": 2017,
      "paper_link": "https://aclanthology.org/J17-3005/",
	  "doi": "https://doi.org/10.1162/coli_a_00295",
	  "open_alex_id": "https://openalex.org/W2343649478",
      "annotations": [
        {
          "corpus_id": "aae_v2",
          "annotation_task": [
            "Argument Component Segmentation", "Argument Component Type Identification"
          ],
          "description": [
		  "Coarse-grained argument component classification:",
		  "402 persuasive essays were segmented into their argumentative components by annotators, which were then classified as either major claim, claim, or premise. Annotation was performed by three non-native speakers, one of which was an expert, and annotation was performed using the brat rapid annotation tool. Annotator agreement was reported for a random subset of 80 essays that all three annotators labelled, a modification of Krippendorf’s α that allows for disagreement in span boundaries was used to capture annotation performance in both the segmentation of argumentative units and their classification."
          ],
          "annotator_type": "3, 1 of which was an expert",
          "agreement_type": "Krippendorff's αU",
          "agreement_score": {"Major Claim": 0.810, "Claim": 0.584, "Premise": 0.824},
          "accessibility": "Free",
          "corpus_link": "https://tudatalib.ulb.tu-darmstadt.de/items/9177c48c-8bd5-4881-9cb4-0632b5941464"
        },
		{
          "corpus_id": "aae_v2",
          "annotation_task": [
            "Argument Relation Identification", "Argument Relation Type Classification"
          ],
          "description": [
		  "Argument component relation classification:",
		  "A set of 4922 previously annotated argumentative units from 402 persuasive essays were annotated with their argumentative relations. Pairs were obtained by considering all pairs of argumentative components that occur within a paragraph as markable. All pairs were annotated by 3 annotators, who could either label relations as support or attack."
          ],
          "annotator_type": "3, 1 of which was an expert",
          "agreement_type": "Fleiss' κ",
          "agreement_score": {"Support": 0.708, "Attack": 0.737},
          "accessibility": "Free",
          "corpus_link": "https://tudatalib.ulb.tu-darmstadt.de/items/9177c48c-8bd5-4881-9cb4-0632b5941464"
        }
      ]
    },	
	{
      "paper_id": "recognizing_the_absence_of_opposing_arguments_in_persuasive_essays",
      "paper_title": "Recognizing the Absence of Opposing Arguments in Persuasive Essays",
      "authors": ["Christian Stab", "Iryna Gurevych"],
      "year": 2016,
      "paper_link": "https://aclanthology.org/W16-2813/",
	  "doi": "https://doi.org/10.18653/v1/w16-2813",
	  "open_alex_id": "https://openalex.org/W2516916351",
      "annotations": [
        {
          "corpus_id": "aae_v2",
          "annotation_task": [
            "Other"
          ],
          "description": [
		  "Myside bias:",
		  "402 persuasive essays are annotated as either positive or negative, where positive indicates the say includes an opposing an argument, and negative if it only includes arguments supporting the author’s standpoint. The existing AAE2 corpus was used to derive initial document-level annotations, previously annotated argument structures include arguments annotated with support or attack relationships within essays and so could be re-used. To verify these automatically derived annotations, a subset of 80 essays were annotated by 3 independent annotators and agreement was reported which showed the derived annotations were reliable."
          ],
          "annotator_type": "3 unspecified annotators",
          "agreement_type": "Krippendorff's α",
          "agreement_score": 0.787,
          "accessibility": "Free",
          "corpus_link": "https://tudatalib.ulb.tu-darmstadt.de/items/ecb6f8cf-0bf3-4843-b299-8c1d9819cee9 "
        }
      ]
    },
	{
      "paper_id": "recognizing_insufficiently_supported_arguments_in_argumentative_essays",
      "paper_title": "Recognizing Insufficiently Supported Arguments in Argumentative Essays",
      "authors": ["Christian Stab", "Iryna Gurevych"],
      "year": 2017,
      "paper_link": "https://aclanthology.org/E17-1092/",
	  "doi": "https://doi.org/10.18653/v1/e17-1092",
	  "open_alex_id": "https://openalex.org/W2740760908",
      "annotations": [
        {
          "corpus_id": "aae_v2",
          "annotation_task": [
            "Minimal Argument Quality Assessment"
          ],
          "description": [
		  "Sufficiency:",
		  "Each paragraph from 402 persuasive essays was considered as an argument, resulting in 1029 arguments ready for annotation. Then, 3 non-native speakers with excellent English proficiency independently annotated all arguments as either sufficient (premises provide enough evidence to accept or reject the claim) or insufficient. An evaluation set of 433 arguments was annotated by all three annotators to compute agreement."
          ],
          "annotator_type": "3 non-native speakers with excellent English proficiency",
          "agreement_type": "Fleiss' κ",
          "agreement_score": 0.7672,
          "accessibility": "Free",
          "corpus_link": "https://tudatalib.ulb.tu-darmstadt.de/items/5d6a075e-697d-4f82-977f-b2cb16aa8542 "
        }
      ]
    },
	{
      "paper_id": "towards_fine_grained_argumentation_strategy_analysis_in_persuasive_essays",
      "paper_title": "Towards Fine-Grained Argumentation Strategy Analysis in Persuasive Essays",
      "authors": ["Robin Schaefer", "René Knaebel", "Manfred Stede"],
      "year": 2023,
      "paper_link": "https://aclanthology.org/2023.argmining-1.8/",
	  "doi": "https://doi.org/10.18653/v1/2023.argmining-1.8",
	  "open_alex_id": "https://openalex.org/W4389518243",
      "annotations": [
        {
          "corpus_id": "aae_v2",
          "annotation_task": [
            "Argument Component Type Classification"
          ],
          "description": [
		  "Fine-grained claim-type classification:",
		  "402 essays were annotated with a fine-grained classification of their claims types. Claims in the essay had previously been extracted. Annotation was performed by 3 experts (including 1 author), who were able to label claims as one of 3 types: policy, value, or fact. Annotation proceeded in stages, with agreement computed at each stage to improve guidelines, final annotator agreement was computed over a set of 40 essays that all annotators labelled."
          ],
          "annotator_type": "3 experts",
          "agreement_type": "Krippendorff's α",
          "agreement_score": {"Policy": 0.78, "Value": 0.52, "Fact": 0.34, "Overall": 0.52},
          "accessibility": "Free",
          "corpus_link": "https://github.com/discourse-lab/arg-essays-semantic-types"
        },
        {
          "corpus_id": "aae_v2",
          "annotation_task": [
            "Argument Component Type Classification"
          ],
          "description": [
		  "Fine-grained premise-type classification:",
		  "402 essays were annotated with a fine-grained classification of their premise types. Premises in the essay had previously been extracted in. Annotation was performed by 3 experts (including 1 author), who were able to label premises as one of 6 types: testimony, statistics, hypothetical-instance, real-example, common-ground, or other. Annotation proceeded in stages, with agreement computed at each stage to improve guidelines, final annotator agreement was computed over a set of 40 essays that all annotators labelled. The number of occurrences of the \"testimony\" and \"other\" class was too small to compute inter-annotator agreement."
          ],
          "annotator_type": "3 experts",
          "agreement_type": "Krippendorff's α",
          "agreement_score": {"Statistics": 0.16, "Hypothetical-Instance": 0.70, "Real-Example": 0.58, "Common-Ground": 0.42, "Overall": 0.53},
          "accessibility": "Free",
          "corpus_link": "https://github.com/discourse-lab/arg-essays-semantic-types"
        }
      ]
    },	
	{
      "paper_id": "graph_embeddings_for_argumentation_quality_assessment",
      "paper_title": "Graph Embeddings for Argumentation Quality Assessment",
      "authors": ["Santiago Marro", "Elena Cabrio", "Serena Villata"],
      "year": 2022,
      "paper_link": "https://aclanthology.org/2022.findings-emnlp.306/",
	  "doi": "https://doi.org/10.18653/v1/2022.findings-emnlp.306",
	  "open_alex_id": "https://openalex.org/W4315703085",
      "annotations": [
        {
          "corpus_id": "aae_v2",
          "annotation_task": [
            "Maximal Argument Quality Assessment"
          ],
          "description": [
		  "Cogency and reasonableness:",
		  "The arguments contained in 402 persuasive essays were annotated with scores for their cogency and reasonableness. An argument is cogent if it has individually acceptable premises that are relevant and sufficient to draw the arguments conclusions. An argument is reasonable if it contributes to the resolution of an issue in a way that is acceptable to the target audience. The previously annotated argument components and relations in the AEE2 corpus were used to aid in annotation. Cogency was scored on a 3-point scale (originally a 5-point scale which was reduced after experimentation) for the premises of a given argument. Reasonableness was computed on a 5-point scale for a whole argumentation graph, taking into consideration the reasonableness of counterarguments and rebuttals. Annotation was carried out by 3 English speakers who were experts in argument mining. Agreement was calculated over a subset of 33 essays."
          ],
          "annotator_type": "3 English speakers who were experts in argument mining",
          "agreement_type": "Fleiss' κ",
          "agreement_score": {"Cogency": 0.86, "Reasonableness Counterargument": 0.78, "Reasonableness Rebuttal": 0.84},
          "accessibility": "Free",
          "corpus_link": "https://aclanthology.org/2022.findings-emnlp.306/"
        },
		{
          "corpus_id": "aae_v2",
          "annotation_task": [
            "Argument Type Identification"
          ],
          "description": [
		  "Rhetorical strategy:",
		  "402 essays were annotated at the argument level with their rhetorical strategy: either ethos, logos, or pathos. Annotation was carried out by 3 English speakers who were experts in argument mining. Agreement was calculated over a subset of 33 essays."
          ],
          "annotator_type": "3 English speakers who were experts in argument mining",
          "agreement_type": "Fleiss' κ",
          "agreement_score": 0.85,
          "accessibility": "Free",
          "corpus_link": "https://aclanthology.org/2022.findings-emnlp.306/"
        }
      ]
    },
	{
      "paper_id": "zero_shot_stance_detection_a_dataset_and_model_using_generalized_topic_representations",
      "paper_title": "Zero-Shot Stance Detection: A Dataset and Model using Generalized Topic Representations",
      "authors": ["Emily Allaway", "Kathleen McKeown"],
      "year": 2020,
      "paper_link": "https://aclanthology.org/2020.emnlp-main.717/",
	  "doi": "https://doi.org/10.18653/v1/2020.emnlp-main.717",
	  "open_alex_id": "https://openalex.org/W3091998006",
      "annotations": [
        {
          "corpus_id": "arc",
          "annotation_task": [
            "Claim Extraction with Stance Classification"
          ],
          "description": [
		  "Stance classification:",
		  "3365 comments on 304 topics are annotated with their stance, a subset of the ARC corpus. Annotators were first asked to list all topics related to the comment and then provided with an automatically generated topic for the comment. Topics were extracted heuristically partly based on the original stance-position annotations in the ARC corpus. If an annotator disagreed that a topic applied to a given comment they could correct it, otherwise each annotator proceeded to annotate the stance of the comment towards the topic. Stance was labelled on 5 point scale, which was then mapped to a 3-point pro/con/neutral label. Each topic-comment pair was annotated by 3 crowd workers (Mechanical Turk), and the gold label taken by majority vote. Poor quality annotations were filtered out manually and by using MACE. The resulting dataset contains three kinds of annotation: stance labels on extracted topics provided to annotators, labels on topics corrected by annotators, and lists of possible topics provided by the annotators for each comment."
          ],
          "annotator_type": "3 crowd workers per topic-comment pair",
          "agreement_type": "Krippendorff's α",
          "agreement_score": 0.427,
          "accessibility": "Free",
          "corpus_link": "https://github.com/emilyallaway/zero-shot-stance"
        }
      ]
    },
	{
      "paper_id": "an_annotated_corpus_of_argumentative_microtexts",
      "paper_title": "An Annotated Corpus of Argumentative Microtexts",
      "authors": ["Andreas Peldszus", "Manfred Stede"],
      "year": 2015,
      "paper_link": "https://peldszus.github.io/files/eca2015-preprint.pdf",
      "annotations": [
        {
          "corpus_id": "microtexts_p1",
          "annotation_task": [
            "Argument Component Segmentation", "Argument Component Type Classification", "Argument Relation Identification", "Argument Relation Type Classification"
          ],
          "description": [
		  "Argumentation structures:",
		  "112 short texts (written in German) were annotated by their argumentation structure based on Freeman’s theory of the macro-structure of argumentation. The annotation scheme argumentative roles, argumentative functions, and the identification of central claims, which together form a fully linked argumentative structure. Annotation was carried out on the original German version of the texts by 3 annotators, and agreement was computed over a subset of 8 texts."
          ],
          "annotator_type": "3 unspecified annotators",
          "agreement_type": "Fleiss' κ",
          "agreement_score": 0.83,
          "accessibility": "Free",
          "corpus_link": "https://angcl.ling.uni-potsdam.de/resources/argmicro.html"
        }
      ]
    },
	{
      "paper_id": "a_multi_layer_annotated_corpus_of_argumentative_text_from_argument_schemes_to_discourse_relations",
      "paper_title": "A Multi-layer Annotated Corpus of Argumentative Text: From Argument Schemes to Discourse Relations",
      "authors": ["Elena Musi", "Tariq Alhindi", "Manfred Stede", "Leonard Kriese", "Smaranda Muresan", "Andrea Rocci"],
      "year": 2018,
      "paper_link": "https://aclanthology.org/L18-1258/",
	  "open_alex_id": "https://openalex.org/W2805780415",
      "annotations": [
        {
          "corpus_id": "microtexts_p1",
          "annotation_task": [
            "Argument Type Identification"
          ],
          "description": [
		  "Argumentum Model of Topics:",
		  "112 short texts were annotated with their argument schemes based on the Argumentum Model of Topics. Annotation involved two tasks, first annotators had to, given a support or rebut relation that already existed in the original Microtexts corpus, identify the argument scheme among 8 possible schemes (or none if no reasoning was present). Second, annotators had to identify the associated inference rule of any identified argument scheme, these inference rules have forms such as \"if the effects the case, the cause is probably the case\" or \"if the cause is the case, the effect is the case\". Annotation was performed by 6 annotators (all students with relevant backgrounds), and agreement was computed over two sets of 20 microtexts that had each been annotated by 3 annotators."
          ],
          "annotator_type": "6 annotators, including 4 students with a background in linguistics and argumentation, and 2 PhD students with a background in natural language processing",
          "agreement_type": "Fleiss' κ",
          "agreement_score": 0.296
          "accessibility": "Free",
          "corpus_link": "https://angcl.ling.uni-potsdam.de/resources/argmicro.html"
        }
      ]
    },
	{
      "paper_id": "more_or_less_controlled_elicitation_of_argumentative_text_Enlarging_a_microtext_corpus_via-crowdsourcing",
      "paper_title": "More or less controlled elicitation of argumentative text: Enlarging a microtext corpus via crowdsourcing",
      "authors": ["Maria Skeppstedt", "Andreas Peldszus", "Manfred Stede."],
      "year": 2018,
      "paper_link": "https://aclanthology.org/W18-5218/",
	  "doi": "https://doi.org/10.18653/v1/w18-5218",
	  "open_alex_id": "https://openalex.org/W2898653207",
      "annotations": [
        {
          "corpus_id": "microtexts_p2",
          "annotation_task": [
            "Argument Component Segmentation, Argument Component Type Classification", "Argument Relation Identification", "Argument Relation Type Classification"
          ],
          "description": [
		  "Argumentation structures:",
		  "171 short texts were annotated with full argumentative structure by 2 annotators. Annotator agreement was not computed, as annotators followed a similar annotation scheme to the first version of the argumentative microtexts corpus. Their annotation resulted in the identification of 932 argumentative units, connected by 467 convergent support relations, 23 example support relations, 137 rebutting attack relations, 77 undercutting attack relations, 57 linked support or attack relations, and 29 restatement relations. Notably, restatement relations did not occur in the first version of the microtexts corpus."
          ],
          "annotator_type": "2 annotators, one who was a co-author of the work",
          "agreement_type": "Not reported",
          "agreement_score": "Not reported",
          "accessibility": "Free",
          "corpus_link": "https://angcl.ling.uni-potsdam.de/resources/argmicro.html"
        }
      ]
    },
	{
      "paper_id": "learning_from_revisions_quality_assessment_of_claims_in_argumentation_at_scale",
      "paper_title": "Learning From Revisions: Quality Assessment of Claims in Argumentation at Scale",
      "authors": ["Gabriella Skitalinskaya", "Jonas Klaff", "Henning Wachsmuth"],
      "year": 2021,
      "paper_link": "https://aclanthology.org/2021.eacl-main.147/",
	  "doi": "https://doi.org/10.18653/v1/2021.eacl-main.147",
	  "open_alex_id": "https://openalex.org/W3122759107",
      "annotations": [
        {
          "corpus_id": "claimrev_v1",
          "annotation_task": [
            "Maximal Argument Quality Assessment"
          ],
          "description": [
		  "Relative quality assessment:",
		  "A randomly sampled set of 315 claim revision pairs was annotated by the authors to test the hypothesis that later versions have an increase in quality to previous versions. Their hypothesis was supported with the results that 93% of later versions had an improved overall argument quality in comparison to previous versions."
          ],
          "annotator_type": "2 authors of the paper (experts)",
          "agreement_type": "Cohen's κ",
          "agreement_score": 0.75,
          "accessibility": "Free",
          "corpus_link": "https://github.com/GabriellaSky/claimrev "
        },
		{
          "corpus_id": "claimrev_v1",
          "annotation_task": [
            "Other"
          ],
          "description": [
		  "Revision label agreement:",
		  "A random sampled set of 440 claim revision pairs were annotated by authors of the paper by their type (such as "Claim Clarification") to compare with original labels provided by kialo. Agreement was reported between the annotators, and between the annotators and the original labels."
          ],
          "annotator_type": "2 authors of the paper (experts)",
          "agreement_type": "Cohen's κ",
          "agreement_score": {"Annotator agreement": 0.84, "Agreement with original labels": 0.82},
          "accessibility": "Free",
          "corpus_link": "https://github.com/GabriellaSky/claimrev "
        },
		{
          "corpus_id": "claimrev_v1",
          "annotation_task": [
            "Maximal Argument Quality Assessment"
          ],
          "description": [
		  "Taxonomical argument quality assessment:",
		  "A randomly sampled set of 315 claim revision pairs was annotated by the authors to explore whether the type of revision had an effect on one of 15 argument quality dimensions. The authors observed strong correlations between some revision types and quality dimensions (such as \"Corrected/Added Links\" and the logical dimensions of \"Cogency\" and \"Local Sufficiency\")."
          ],
          "annotator_type": "1 author of the paper (expert)",
          "agreement_type": "None",
          "agreement_score": "None",
          "accessibility": "Free",
          "corpus_link": "https://github.com/GabriellaSky/claimrev "
        }
      ]
    },
	{
      "paper_id": "claim_optimization_in_computational_argumentation",
      "paper_title": "Claim Optimization in Computational Argumentation",
      "authors": ["Gabriella Skitalinskaya", "Maximilian Spliethöver", "Henning Wachsmuth"],
      "year": 2023,
      "paper_link": "https://aclanthology.org/2023.inlg-main.10/",
	  "doi": "https://doi.org/10.18653/v1/2023.inlg-main.10",
	  "open_alex_id": "https://openalex.org/W4389009581",
      "annotations": [
        {
          "corpus_id": "claimrev_v2",
          "annotation_task": [
            "None"
          ],
          "description": [
		  "Unannotated"
          ],
          "annotator_type": "Unannotated",
          "agreement_type": "None",
          "agreement_score": "None",
          "accessibility": "Free",
          "corpus_link": "https://github.com/GabriellaSky/claim_optimization"
        }
      ]
    },
	{
      "paper_id": "a_corpus_of_erulemaking_user_comments_for_measuring_evaluability_of_arguments",
      "paper_title": "A Corpus of eRulemaking User Comments for Measuring Evaluability of Arguments",
      "authors": ["Joonsuk Park", "Claire Cardie"],
      "year": 2018,
      "paper_link": "https://aclanthology.org/L18-1257/",
	  "open_alex_id": "https://openalex.org/W2807191971,
      "annotations": [
        {
          "corpus_id": "cornell_erulemaking",
          "annotation_task": [
            "Argument Component Type Classification"
          ],
          "description": [
		  "Elementary unit classification:",
		  "A dataset of 731 comments were annotated at the sentence/clause level with their argument components, resulting in 4931 argument components in the final labelling. Argument components were one of 4 \"Elementary Units\": fact, testimony, value, policy, or reference. Annotation was performed by 2 annotators, while a 3rd annotator was used to resolve disagreements and produce the final gold label. Annotator agreement was reported between the 2 primary annotators."
          ],
          "annotator_type": "2 unspecified annotators",
          "agreement_type": "Krippendorff's α",
          "agreement_score": 0.648,
          "accessibility": "Free",
          "corpus_link": "https://huggingface.co/datasets/DFKI-SLT/cdcp"
        },
		{
          "corpus_id": "cornell_erulemaking",
          "annotation_task": [
            "Argument Relation Identification", "Argument Relation Type Classification"
          ],
          "description": [
		  "Support relation classification:",
		  "A dataset of 731 comments that were previously annotated with their argument components, were annotated again with the relation between their argument components, resulting in 1221 relations in the final labelling. Argument relations were one of 2 support relations: reason or evidence. Annotation was performed by 2 annotators, while a 3rd annotator was used to resolve disagreements and produce the final gold label. Annotator agreement was reported between the 2 primary annotators."
          ],
          "annotator_type": "2 unspecified annotators",
          "agreement_type": "Krippendorff's α",
          "agreement_score": 0.441,
          "accessibility": "Free",
          "corpus_link": "https://huggingface.co/datasets/DFKI-SLT/cdcp"
        }
      ]
    },
	{
      "paper_id": "an_argument_annotated_corpus_of_scientific_publications",
      "paper_title": "An Argument-Annotated Corpus of Scientific Publications",
      "authors": ["Anne Lauscher", "Goran Glavaš", "Simone Paolo Ponzetto"],
      "year": 2018,
      "paper_link": "https://aclanthology.org/W18-5206/",
	  "doi": "https://doi.org/10.18653/v1/w18-5206",
	  "open_alex_id": "https://openalex.org/W2898734253",
      "annotations": [
        {
          "corpus_id": "dr_inventor",
          "annotation_task": [
            "Argument Component Type Classification"
          ],
          "description": [
		  "Toulmin-style component classification:",
		  "Scientific publications were annotated with their argumentative components by 4 annotators (1 expert and 3 non-expert). Component types were based on the Toulmin model and a preliminary study, resulting in three argumentative components: own claim, background claim, and data. Annotator agreement was computed using F1, and was reported in a bar chart that gives approximate values. Annotator agreement was computed for 5 iterations of the task, and for both a weak and strict interpretation of the spans of the argument components."
          ],
          "annotator_type": "1 expert and 3 non-expert annotators",
          "agreement_type": "F1",
          "agreement_score": {"Approximate 5th iteration weak": 0.75 "Approximate 5th iteration strict": 0.6},
          "accessibility": "Free",
          "corpus_link": "http://data.dws.informatik.uni-mannheim.de/sci-arg/compiled_corpus.zip"
        },
		{
          "corpus_id": "dr_inventor",
          "annotation_task": [
            "Argument Relation Identification", "Argument Relation Type Classification"
          ],
          "description": [
		  "Dung-style relation classification:",
		  "previously annotated argument components from 40 scientific publications were annotated with their relation to each other by 4 annotators (1 expert and 3 non-expert). Relations were based on those described in Dung (1995) and included the three relations of supports, contradicts, and semantically the same (argument coreference). Annotator agreement was computed using F1, and was reported in a bar chart that gives approximate values. Annotator agreement was computed for 5 iterations of the task, and for both a weak and strict interpretation of the spans of the argument components."
          ],
          "annotator_type": "1 expert and 3 non-expert annotators",
          "agreement_type": "F1",
          "agreement_score": {"Approximate 5th iteration weak": 0.45, "Approximate 5th iteration strict": 0.35},
          "accessibility": "Free",
          "corpus_link": "http://data.dws.informatik.uni-mannheim.de/sci-arg/compiled_corpus.zip"
        }
      ]
    },
	{
      "paper_id": "ethix_a_dataset_for_argument_scheme_classification_in_ethical_debates",
      "paper_title": "EthiX: A Dataset for Argument Scheme Classification in Ethical Debates",
      "authors": ["Elfia Bezou-Vrakatseli", "Oana Cocarascu", "Sanjay Modgil"],
      "year": 2024,
      "paper_link": "https://ebooks.iospress.nl/DOI/10.3233/FAIA240919",
	  "doi": "https://doi.org/10.3233/faia240919",
	  "open_alex_id": "https://openalex.org/W4403487202",
      "annotations": [
        {
          "corpus_id": "ethix",
          "annotation_task": [
            "Argument Type Identification"
          ],
          "description": [
		  "Argument scheme identification:",
		  "Arguments are annotated arguments with their argument scheme according to Walton's taxonomy (e.g., argument from example, values, positive/negative consequences, cause-to-effect, expert opinion, alternatives, analogy), with Argument Scheme Key (ASK) guidelines to aid consistent labeling. Annotation was performed by the authors, and agreement measured with another PhD student who was not an expert in argumentation."
          ],
          "annotator_type": "Authors and a PhD student in explainable AI, trained with workshops and examples",
          "agreement_type": "Cohen's κ",
          "agreement_score": 0.523,
          "accessibility": "Free",
          "corpus_link": "https://github.com/ElfiaBv/EthiX"
        }
      ]
    },
	{
      "paper_id": "creating_a_domain_diverse_corpus_for_theory_based_argument_quality_assessment",
      "paper_title": "Creating a Domain-diverse Corpus for Theory-based Argument Quality Assessment",
      "authors": ["Lily Ng", "Anne Lauscher", "Joel Tetreault", "Courtney Napoles"],
      "year": 2020,
      "paper_link": "https://aclanthology.org/2020.argmining-1.13/",
	  "doi": "https://doi.org/10.48550/arxiv.2011.01589",
	  "open_alex_id": "https://openalex.org/W4287608578",
      "annotations": [
        {
          "corpus_id": "gaq",
          "annotation_task": [
            "Maximal Argument Quality Assessment"
          ],
          "description": [
		  "Theory-driven argument quality assessment: ",
		  "Arguments are annotated along 3 argument quality dimensions (cogency, reasonableness, and effectiveness) as well as for overall quality along a 5 point scale (very low/low/medium/high/very high quality or cannot judge). Subdimensions of the 3 argument quality dimensions are not scored, but are used to guide annotation. Annotation was performed by two groups, a set of four expert annotators with a deep understanding of argumentation theory, and a set of crowd annotators who were fluent in English. Agreement was measured along each quality dimension by comparing annotations between the set of experts and set of crowd annotators in arguments that were annotated by both groups within each category of text (CQA or debate or review)."
          ],
          "annotator_type": "4 expert annotators, and an unspecified number of crowd annotators",
          "agreement_type": "Krippendorff's α",
          "agreement_score": {"CQA Cogency": 0.42, "CQA Effectiveness": 0.52, "CQA Reasonableness": 0.52, "CQA Overall": 0.53,
				"Debates Cogency": 0.14, "Debates Effectiveness": 0.11, "Debates Reasonableness": 0.21, "Debates Overall": 0.19,
				"Reviews Cogency": 0.32, "Reviews Effectiveness": 0.32, "Reviews Reasonableness": 0.31, "Reviews Overall": 0.33
			},
          "accessibility": "Free",
          "corpus_link": "https://github.com/grammarly/gaqcorpus "
        }
      ]
    },
	{
      "paper_id": "a_large_scale_dataset_for_argument_quality_ranking_construction_and_analysis",
      "paper_title": "A Large-scale Dataset for Argument Quality Ranking: Construction and Analysis",
      "authors": ["Shai Gretz", "Roni Friedman", "Edo Cohen-Karlik", "Assaf Toledo", "Dan Lahav", "Ranit Aharonov", "Noam Slonim"],
      "year": 2020,
      "paper_link": "https://ojs.aaai.org/index.php/AAAI/article/view/6285",
	  "doi": "https://doi.org/10.1609/aaai.v34i05.6285",
	  "open_alex_id": "https://openalex.org/W2991389267",
      "annotations": [
        {
          "corpus_id": "ibm_rank_30k",
          "annotation_task": [
            "Maximal Argument Quality Assessment"
          ],
          "description": [
		  "Individual Binary Argument Quality:",
		  "Arguments from are annotated with a binary quality score (1 or 0) by crowd annotators. 10 annotators annotated each argument, with an answer of yes/no (1 or 0) if they would recommend the argument be used as is by a person in a hypothetical speech on the topic. Poorly performing annotators, and those who failed test questions, were filtered out. To obtain a continuous quality score, scores were aggregated using two methods, MACE-P and WA (details in paper). Annotator agreement by crowd annotators is not reported in the paper, however the continuous scoring functions (MACE-P and WA) were assessed using several methods that compare predicted scores to a gold-standard pairwise annotation."
          ],
          "annotator_type": "10 crowd annotators per argument",
          "agreement_type": "Not reported",
          "agreement_score": "Not reported",
          "accessibility": "Free",
          "corpus_link": "https://huggingface.co/datasets/ibm-research/argument_quality_ranking_30k "
        }
      ]
    },
	{
      "paper_id": "a_benchmark_dataset_for_automatic_detection_of_claims_and-evidence_in_the_context_of_controversial_topics",
      "paper_title": "A Benchmark Dataset for Automatic Detection of Claims and Evidence in the Context of Controversial Topics",
      "authors": ["Ehud Aharoni", "Anatoly Polnarov", "Tamar Lavee", "Daniel Hershcovich", "Ran Levy", "Ruty Rinott", "Dan Gutfreund", "Noam Slonim"],
      "year": 2014,
      "paper_link": "https://aclanthology.org/W14-2109/",
	  "doi": "https://doi.org/10.3115/v1/w14-2109",
	  "open_alex_id": "https://openalex.org/W2251448848",
      "annotations": [
        {
          "corpus_id": "ibm_ce_v1",
          "annotation_task": [
            "Argument Component Segmentation", "Argument Component Identification", "Argument Summarization"
          ],
          "description": [
		  "Claim and evidence identification:",
		  "586 Wikipedia articles were annotated to derive a set of 2683 argument elements made up of context-dependent claims and context-dependent evidence. The annotation process was meticulous, and involved 20 well-trained annotators who followed several steps. First, 5 annotators in a search team were given a topic and independently searched for related articles. Second, 5 annotators independently detected candidate context-dependent claims from the articles. Third, in a \"Claim confirmation\" stage, 5 annotators examined these candidates and rejected/accepted them (agreement was reported for this stage). In a fourth stage, 5 annotators detected candidate context-dependent evidence related to accepted context-dependent claims. In a fifth stage, \"Evidence confirmation\" was performed in the same manner as \"Claim confirmation\", with 5 annotators accepting/rejecting candidate context-dependent evidence."
          ],
          "annotator_type": "20 well-trained annotators",
          "agreement_type": "Cohen's κ",
          "agreement_score": {"Claim confirmation": 0.39, "Evidence confirmation": 0.4},
          "accessibility": "Free",
          "corpus_link": "https://research.ibm.com/haifa/dept/vst/debating_data.shtml"
        }
      ]
    },
	{
      "paper_id": "show_me_your_evidence_an_automatic_method_for_context_dependent_evidence_detection",
      "paper_title": "Show Me Your Evidence - an Automatic Method for Context Dependent Evidence Detection",
      "authors": ["Ruty Rinott", "Lena Dankin", "Carlos Alzate Perez", "Mitesh M. Khapra", "Ehud Aharoni", "Noam Slonim"],
      "year": 2015,
      "paper_link": "https://aclanthology.org/D15-1050/",
	  "doi": "https://doi.org/10.18653/v1/d15-1050",
	  "open_alex_id": "https://openalex.org/W2250762536",
      "annotations": [
        {
          "corpus_id": "ibm_ce_v2",
          "annotation_task": [
            "Argument Component Segmentation", "Argument Component Identification", "Argument Summarization"
          ],
          "description": [
		  "Evidence identification:",
		  "547 Wikipedia articles were annotated to derive a set of context-dependent evidence. In the first stage, 5 annotators read the article and marked all context-dependent evidence candidates, in a second stage, 5 annotators confirmed or rejected each candidate and performed a fine-grained classification of the type of evidence (study, expert or anecdotal). Context-dependent evidence candidates and their type were determined by majority vote."
          ],
          "annotator_type": "15 annotators involved across all stages",
          "agreement_type": "Not reported",
          "agreement_score": "Not reported",
          "accessibility": "Free",
          "corpus_link": "https://research.ibm.com/haifa/dept/vst/debating_data.shtml"
        }
      ]
    },
	{
      "paper_id": "",
      "paper_title": "",
      "authors": [""],
      "year": ,
      "paper_link": "",
	  "doi": "",
	  "open_alex_id": "",
      "annotations": [
        {
          "corpus_id": "ibm_cs",
          "annotation_task": [
            ""
          ],
          "description": [
		  "",
		  ""
          ],
          "annotator_type": "",
          "agreement_type": "",
          "agreement_score": ,
          "accessibility": "",
          "corpus_link": ""
        }
      ]
    },
	
	
	
	
	{
      "paper_id": "",
      "paper_title": "",
      "authors": [""],
      "year": ,
      "paper_link": "",
	  "doi": "",
	  "open_alex_id": "",
      "annotations": [
        {
          "corpus_id": "",
          "annotation_task": [
            ""
          ],
          "description": [
		  "",
		  ""
          ],
          "annotator_type": "",
          "agreement_type": "",
          "agreement_score": ,
          "accessibility": "",
          "corpus_link": ""
        }
      ]
    },
]
}
